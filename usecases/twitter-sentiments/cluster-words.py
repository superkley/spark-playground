# Jorge Castanon, October 2015
# Data Scientist @ IBM

# run in terminal sitting on YOUR-PATH-TO-REPO:
# ~/Documents/spark-1.5.1/bin/spark-submit mllib-scripts/cluster-words.py
# Replace this line with:
# /YOUR-SPARK-HOME/bin/spark-submit mllib-scripts/cluster-words.py

import numpy as np
import math

from pyspark.context import SparkContext
from pyspark.mllib.clustering import KMeans

# next 2 lines can be reaplced to read from hdfs, 
# if the Word2Vec matrix is big 
Feat = np.load('mllib-scripts/myW2Vmatrix.npy')    # reads model generated by Word2Vec
words = np.load('mllib-scripts/myWordList.npy')    # reads list of words

print "\n================================================="
print "Size of the Word2Vec matrix is: ", Feat.shape 
print "Number of words in the models: ", words.shape
print "=================================================\n"

## Spark Context
sc = SparkContext('local','cluster-words') 

## Read the Word2Vec model
# the next line should be read/stored from hdfs if it is large
Feat = sc.parallelize(Feat) 

## K-means clustering with Spark
K = int(math.floor(math.sqrt(float(words.shape[0])/2))) # Number of clusters
         # K ~ sqrt(n/2) this is a rule of thumb for choosing K,
         # where n is the number of words in the model
         # feel free to choose K with a fancier method
maxiters = 100 # may change depending on the data        
clusters = KMeans.train(Feat, k = K, maxIterations = maxiters, runs = 10) 

print "\n================================================="
print "Number of clusters used: ", K
print "=================================================\n"

## Getting Cluster Labels for each Word and saving to a numpy file
labels =  Feat.map(lambda point: clusters.predict(point)) # add labels to each vector (word)
list_labels = labels.collect()
np.save('mllib-scripts/myClusters.npy',list_labels)

sc.stop()

