{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Setup Notebook for Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "##### <span style=\"color:red\">IMPORTANT: Only modify cells which have the following comment:</span>\n",
    "```python\n",
    "# Modify this cell\n",
    "```\n",
    "##### <span style=\"color:red\">Do not add any new cells when you submit the homework</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Creating the Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local[4]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "import Tester.SparkSQL as SparkSQL\n",
    "pickleFile=\"Tester/SparkSQL.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Creating the SQL Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Just like using Spark requires having a SparkContext, using SQL requires an SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataframes \n",
    "Dataframes are a special type of RDDs. They are similar to, but not the same as, pandas dataframes. They are used to store two dimensional data, similar to the type of data stored in a spreadsheet. Each column in a dataframe can have a different type and each row contains a `record`.\n",
    "\n",
    "Spark DataFrames are similar to `pandas` DataFrames. With the important difference that spark DataFrames are **distributed** data structures, based on RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Exercise 1 -- Creating and transforming dataframes from JSON files\n",
    "\n",
    "[JSON](http://www.json.org/) is a very popular readable file format for storing structured data.\n",
    "Among it's many uses are **twitter**, `javascript` communication packets, and many others. In fact this notebook file (with the extension `.ipynb` is in json format. JSON can also be used to store tabular data and can be easily loaded into a dataframe.\n",
    "\n",
    "In this exercise, you will do the following:\n",
    "\n",
    "* Read the dataset from a json file and store it in a dataframe\n",
    "* Filter the rows which has the column \"make_is_common\" equal to 1\n",
    "* Group the rows by make_country column and compute the count for each country\n",
    "* Return the list of countries which have count greater than n\n",
    "\n",
    "######  <span style=\"color:blue\">Sample Input:</span>\n",
    "```python\n",
    " \n",
    "example.json has following contents\n",
    "\n",
    "        {\"make_id\":\"acura\",\"make_display\":\"Acura\",\"make_is_common\":\"0\",\"make_country\":\"USA\"}\n",
    "        {\"make_id\":\"alpina\",\"make_display\":\"Alpina\",\"make_is_common\":\"1\",\"make_country\":\"UK\"}\n",
    "        {\"make_id\":\"aston-martin\",\"make_display\":\"Aston Martin\",\"make_is_common\":\"1\",\"make_country\":\"UK\"}\n",
    "   \n",
    "country_list = get_country_list(\"example.json\", 1, sqlContext)\n",
    "\n",
    "```\n",
    "######  <span style=\"color:magenta\">Sample Output:</span>\n",
    "country_list = ['UK']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def get_country_list(json_filepath, n, sqlContext):\n",
    "    makes_df = sqlContext.read.json(json_filepath)\n",
    "    \n",
    "    # check the schema of the json file, uncomment the next line to see the schema\n",
    "    #makes_df.printSchema()\n",
    "    \n",
    "    # The scheme should look like the one below\n",
    "    #root\n",
    "    # |-- make_country: string (nullable = true)\n",
    "    # |-- make_display: string (nullable = true)\n",
    "    # |-- make_id: string (nullable = true)\n",
    "    # |-- make_is_common: string (nullable = true)\n",
    "    \n",
    "    sqlContext.registerDataFrameAsTable(makes_df, \"makes_table\") \n",
    "    query = \"SELECT make_country, COUNT(make_country) as count \\\n",
    "             FROM makes_table WHERE make_is_common == '1'\\\n",
    "             GROUP BY make_country HAVING count > \" + str(n)\n",
    "    country_rdd = sqlContext.sql(query).rdd.map(lambda row: row.make_country)\n",
    "    country_list = country_rdd.collect()\n",
    "    return country_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Output: ['Germany', 'Italy', 'USA', 'UK', 'Japan']\n",
      "Great Job!\n"
     ]
    }
   ],
   "source": [
    "import Tester.SparkSQL as SparkSQL\n",
    "SparkSQL.exercise_1(sqlContext, pickleFile, get_country_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 2 -- Creating and transforming dataframes from Parquet files\n",
    "[Parquet](http://parquet.apache.org/) is a columnar format that is supported by many other data processing systems. Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. \n",
    "\n",
    "\n",
    "In this exercise, you will do the following:\n",
    "\n",
    "* Read the dataset from a parquet file and store it in a dataframe\n",
    "* Write a SQL query to group the rows by make_country and compute the count for each make_country\n",
    "* Sort the make_country based on the count in descending order\n",
    "* Return the list of tuples (country, count) of top \"n\" make_country\n",
    "\n",
    "######  <span style=\"color:blue\">Sample Input:</span>\n",
    "```python\n",
    " \n",
    "example.parquet has contents similar to the following json data\n",
    "\n",
    "        {\"make_id\":\"a\",\"make_display\":\"A\",\"make_is_common\":\"0\",\"make_country\":\"USA\"}\n",
    "        {\"make_id\":\"b\",\"make_display\":\"B\",\"make_is_common\":\"1\",\"make_country\":\"UK\"}\n",
    "        {\"make_id\":\"c\",\"make_display\":\"C\",\"make_is_common\":\"1\",\"make_country\":\"UK\"}\n",
    "        {\"make_id\":\"d\",\"make_display\":\"D\",\"make_is_common\":\"1\",\"make_country\":\"USA\"}\n",
    "        {\"make_id\":\"e\",\"make_display\":\"E\",\"make_is_common\":\"0\",\"make_country\":\"Germany\"}\n",
    "        {\"make_id\":\"f\",\"make_display\":\"F\",\"make_is_common\":\"0\",\"make_country\":\"UK\"}\n",
    "   \n",
    "top_n_country_list = get_top_n_country_list(\"example.parquet\", 2, sqlContext)\n",
    "\n",
    "```\n",
    "######  <span style=\"color:magenta\">Sample Output:</span>\n",
    "top_n_country_list = [ ('UK', 3), ('USA', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Modify this cell\n",
    "def get_top_n_country_list(parquet_path, n, sqlContext):\n",
    "    makes_df = sqlContext.read.load(parquet_path)\n",
    "    \n",
    "    # The scheme should look like the one below\n",
    "    #root\n",
    "    # |-- make_country: string (nullable = true)\n",
    "    # |-- make_display: string (nullable = true)\n",
    "    # |-- make_id: string (nullable = true)\n",
    "    # |-- make_is_common: string (nullable = true)\n",
    "    \n",
    "    makes_df.registerTempTable(\"makes_table\")\n",
    "    query = \"SELECT make_country, COUNT(make_country) as make_count \\\n",
    "             FROM makes_table GROUP BY make_country \\\n",
    "             ORDER BY make_count DESC\"\n",
    "    query_result_df = sqlContext.sql(query)\n",
    "    country_rdd = query_result_df.rdd.map(lambda row: (row.make_country, row.make_count))\n",
    "    top_n_country_list = country_rdd.collect()[:n]\n",
    "    return top_n_country_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Output: [('USA', 16), ('Japan', 10), ('UK', 8)]\n",
      "Great Job!\n"
     ]
    }
   ],
   "source": [
    "import Tester.SparkSQL as SparkSQL\n",
    "SparkSQL.exercise_2(sqlContext, pickleFile, get_top_n_country_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Exercise 3 -- Creating and transforming dataframes from CSV files\n",
    "\n",
    "\n",
    "In this exercise, you will do the following:\n",
    "\n",
    "* Read the dataset from a csv file and store it in a dataframe\n",
    "* Filter the rows which has the word \"city\" in the first column of csv file - \"name\" and return the count\n",
    "\n",
    "######  <span style=\"color:blue\">Sample Input:</span>\n",
    "```python\n",
    " \n",
    "example.csv has contents similar to the following csv data \n",
    "\n",
    "    name, country, subcountry, geonameid\n",
    "    logan city,australia,queensland,7281838\n",
    "    carindale,australia,queensland,7281839\n",
    "   \n",
    "city_count = get_city_count(\"example.csv\", sqlContext)\n",
    "\n",
    "```\n",
    "######  <span style=\"color:magenta\">Sample Output:</span>\n",
    "city_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def get_city_count(csv_filepath, sqlContext):\n",
    "    city_df = sqlContext.read.csv(csv_filepath)\n",
    "    count = city_df.filter(\"_c0 like \\'%city%\\'\").count()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Output: 15\n",
      "Great Job!\n"
     ]
    }
   ],
   "source": [
    "import Tester.SparkSQL as SparkSQL\n",
    "SparkSQL.exercise_3(sqlContext, pickleFile, get_city_count)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "261px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
