{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "TF-IDF（term frequency–inverse document frequency）是一种用于信息检索与数据挖掘的常用加权技术。  \n",
    "TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。  \n",
    "逆向文件频率（inverse document frequency，IDF）是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。\n",
    "$$ IDF(t, D) = \\log \\frac{|D| + 1}{DF(t, D) + 1}, $$\n",
    "\n",
    "|D|：语料库中的文件总数。包含词语的文件数目（即的文件数目）如果该词语不在语料库中，就会导致分母为零，因此一般情况下使用 ${DF(t, D) + 1}$ 作为分母。  \n",
    "$$ TFIDF(t, d, D) = TF(t, d) \\cdot IDF(t, D). $$  \n",
    "然后再计算TF与IDF的乘积。  \n",
    "某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语\n",
    "There are several variants on the definition of term frequency and document frequency. In MLlib, we separate TF and IDF to make them flexible.  \n",
    "\n",
    "\n",
    "Our implementation of term frequency utilizes the hashing trick. A raw feature is mapped into an index (term) by applying a hash function. Then term frequencies are calculated based on the mapped indices. This approach avoids the need to compute a global term-to-index map, which can be expensive for a large corpus, but it suffers from potential hash collisions, where different raw features may become the same term after hashing. To reduce the chance of collision, we can increase the target feature dimension, i.e., the number of buckets of the hash table. The default feature dimension is $2^{20} = 1,048,576$    \n",
    "\n",
    "Note: MLlib doesn’t provide tools for text segmentation. We refer users to the Stanford NLP Group and scalanlp/chalk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF and IDF are implemented in HashingTF and IDF. HashingTF takes an RDD[Iterable[_]] as the input. Each record could be an iterable of strings or other types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.mllib.feature.HashingTF\n",
    "import org.apache.spark.mllib.linalg.Vector\n",
    "\n",
    "val sc: SparkContext = ...\n",
    "\n",
    "// Load documents (one per line).\n",
    "val documents: RDD[Seq[String]] = sc.textFile(\"...\").map(_.split(\" \").toSeq)\n",
    "\n",
    "val hashingTF = new HashingTF()\n",
    "val tf: RDD[Vector] = hashingTF.transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While applying HashingTF only needs a single pass to the data, applying IDF needs two passes: first to compute the IDF vector and second to scale the term frequencies by IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature.IDF\n",
    "\n",
    "// ... continue from the previous example\n",
    "tf.cache()\n",
    "val idf = new IDF().fit(tf)\n",
    "val tfidf: RDD[Vector] = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib’s IDF implementation provides an option for ignoring terms which occur in less than a minimum number of documents. In such cases, the IDF for these terms is set to 0. This feature can be used by passing the minDocFreq value to the IDF constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature.IDF\n",
    "\n",
    "// ... continue from the previous example\n",
    "tf.cache()\n",
    "val idf = new IDF(minDocFreq = 2).fit(tf)\n",
    "val tfidf: RDD[Vector] = idf.transform(tf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.5.2 (Scala 2.10)",
   "language": "",
   "name": "spark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
