{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "Word2Vec computes distributed vector representation of words. The main advantage of the distributed representations is that similar words are close in the vector space, which makes generalization to novel patterns easier and model estimation more robust. Distributed vector representation is showed to be useful in many natural language processing applications such as named entity recognition, disambiguation, parsing, tagging and machine translation.  \n",
    "## Model\n",
    "In our implementation of Word2Vec, we used skip-gram model. The training objective of skip-gram is to learn word vector representations that are good at predicting its context in the same sentence. Mathematically, given a sequence of training words w1,w2,…,wTw1,w2,…,wT, the objective of the skip-gram model is to maximize the average log-likelihood   \n",
    "$$ \\frac{1}{T} \\sum_{t = 1}^{T}\\sum_{j=-k}^{j=k} \\log p(w_{t+j} | w_t) $$\n",
    "\n",
    "where k is the size of the training window.\n",
    "\n",
    "In the skip-gram model, every word w is associated with two vectors uw and vw which are vector representations of w as word and context respectively. The probability of correctly predicting word wi given word wj is determined by the softmax model, which is \n",
    "$$ p(w_i | w_j ) = \\frac{\\exp(u_{w_i}^{\\top}v_{w_j})}{\\sum_{l=1}^{V} \\exp(u_l^{\\top}v_{w_j})} $$\n",
    "where V is the vocabulary size.\n",
    "\n",
    "The skip-gram model with softmax is expensive because the cost of computing $\\log p(w_i | w_j)$ is proportional to V, which can be easily in order of millions. To speed up training of Word2Vec, we used hierarchical softmax, which reduced the complexity of computing of $\\log p(w_i | w_j)$ to $O(\\log(V))$  \n",
    "\n",
    "## Example\n",
    "The example below demonstrates how to load a text file, parse it as an RDD of Seq[String], construct a Word2Vec instance and then fit a Word2VecModel with the input data. Finally, we display the top 40 synonyms of the specified word. To run the example, first download the text8 data and extract it to your preferred directory. Here we assume the extracted file is text8 and in same directory as you run the spark shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.rdd._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}\n",
    "\n",
    "val PATH = \"file:///Users/lzz/work/SparkML/\"\n",
    "val input = sc.textFile( PATH + \"data/text8\" ).map(line => line.split(\" \").toSeq)\n",
    "\n",
    "val word2vec = new Word2Vec()\n",
    "\n",
    "val model = word2vec.fit(input)\n",
    "\n",
    "val synonyms = model.findSynonyms(\"china\", 40)\n",
    "\n",
    "for((synonym, cosineSimilarity) <- synonyms) {\n",
    "  println(s\"$synonym $cosineSimilarity\")\n",
    "}\n",
    "\n",
    "// Save and load model\n",
    "// model.save(sc, \"myModelPath\")\n",
    "// val sameModel = Word2VecModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.5.2 (Scala 2.10)",
   "language": "",
   "name": "spark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
