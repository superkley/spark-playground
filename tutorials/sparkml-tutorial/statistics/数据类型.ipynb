{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在单机模式下机器学习库支持本地向量和矩阵存储，以及分布式矩阵由一个或多个RDD组成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local vector 单机模式下支持两种类型dense 和 sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dv:[1.0,0.0,3.0]\n",
      "sv1:(3,[0,2],[1.0,3.0])\n",
      "sv2:(3,[0,2],[1.0,3.0])\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "\n",
    "// Create a dense vector (1.0, 0.0, 3.0) 元素的值是double类型\n",
    "val dv: Vector = Vectors.dense(1.0, 0.0, 3.0)\n",
    "println( \"dv:\"+ dv )\n",
    "// Create a sparse vector (1.0, 0.0, 3.0) 其中第一个数组是索引，第二个数组是索引对应的值\n",
    "val sv1: Vector = Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0))\n",
    "println( \"sv1:\" + sv1 )\n",
    "// Create a sparse vector (1.0, 0.0, 3.0) Seq中的每个元素包含着（索引，值）\n",
    "val sv2: Vector = Vectors.sparse(3, Seq((0, 1.0), (2, 3.0)))\n",
    "println( \"sv2:\" + sv2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeled point 在有监督学习算法中我们对数据用double类型的数据进行标记，用于分类或则回归\n",
    "如果是二分类我们可以使用0 (negative) or 1 (positive)，如果是多类型分类那么我们可以使用0，1，2，3……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0,[1.0,0.0,3.0])\n",
      "(0.0,(3,[0,2],[1.0,3.0]))\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "\n",
    "// Create a labeled point with a positive label and a dense feature vector.\n",
    "val pos = LabeledPoint(1.0, Vectors.dense(1.0, 0.0, 3.0))\n",
    "println(pos)\n",
    "// Create a labeled point with a negative label and a sparse feature vector.\n",
    "val neg = LabeledPoint(0.0, Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0)))\n",
    "println(neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是mllib中labeled point样本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((0.0,(692,[127,128,129,130,131,154,155,156,157,158,159,181,182,183,184,185,186,187,188,189,207,208,209,210,211,212,213,214,215,216,217,235,236,237,238,239,240,241,242,243,244,245,262,263,264,265,266,267,268,269,270,271,272,273,289,290,291,292,293,294,295,296,297,300,301,302,316,317,318,319,320,321,328,329,330,343,344,345,346,347,348,349,356,357,358,371,372,373,374,384,385,386,399,400,401,412,413,414,426,427,428,429,440,441,442,454,455,456,457,466,467,468,469,470,482,483,484,493,494,495,496,497,510,511,512,520,521,522,523,538,539,540,547,548,549,550,566,567,568,569,570,571,572,573,574,575,576,577,578,594,595,596,597,598,599,600,601,602,603,604,622,623,624,625,626,627,628,629,630,651,652,653,654,655,656,657],[51.0,159.0,..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val examples: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, \"../data/mllib/sample_libsvm_data.txt\")\n",
    "examples.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Local matrix 本地矩阵中的每个元素都是以double类型进行存储，机器学习库支持两种类型的local matrix, DenseMatrix, and SparseMatrix\n",
    " 。下面是我们用array [1.0, 3.0, 5.0, 2.0, 4.0, 6.0] 转化为一个3行2列的DenseMatrix\n",
    " $$\\begin{pmatrix}\n",
    "1.0 & 2.0 \\\\\n",
    "3.0 & 4.0 \\\\\n",
    "5.0 & 6.0\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dm:\n",
      "1.0  2.0  \n",
      "3.0  4.0  \n",
      "5.0  6.0  \n",
      "sm:3 x 2 CSCMatrix\n",
      "(0,0) 9.0\n",
      "(2,1) 6.0\n",
      "(1,1) 8.0\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.{Matrix, Matrices}\n",
    "\n",
    "// Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0)) 3行2列matrix按列优先进行存放\n",
    "val dm: Matrix = Matrices.dense(3, 2, Array(1.0, 3.0, 5.0, 2.0, 4.0, 6.0))\n",
    "println(\"dm:\")\n",
    "println(dm)\n",
    "// Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0)) 第一个数组表示的是矩阵的列下标，第二个表示的是行下标\n",
    "val sm: Matrix = Matrices.sparse(3, 2, Array(0, 1, 3), Array(0, 2, 1), Array(9, 6, 8))\n",
    "println(\"sm:\"+sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Distributed matrix 分布式矩阵的行和列都是用long类型保存的，元素值使用double类型进行存储，选择合适的类型分布式矩阵来存储数据是很重要的因为矩阵类型的转化是全局shuffle所以代价很高。\n",
    "Ddistributed matrix 分为四种类型RowMatrix IndexedRowMatrix CoordinateMatrix BlockMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RowMatrix 中的每一行都是有一个local vector进行存储，列的数目是被限制住一个整数范围内"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m=4 n=3\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val ad_tags = Seq(\n",
    "    Vectors.dense( 3, 1,1),\n",
    "    Vectors.dense( 5, 2,1),\n",
    "    Vectors.dense( 6, 3,1),\n",
    "    Vectors.dense( 7, 5,1)\n",
    "  )\n",
    "\n",
    "  val rows: RDD[Vector] = sc.parallelize( ad_tags )\n",
    "  // Create a RowMatrix from an RDD[Vector].\n",
    "  val mat: RowMatrix = new RowMatrix(rows)\n",
    "  // Get its size.\n",
    "  val m = mat.numRows()\n",
    "  val n = mat.numCols()\n",
    "  println( \"m=\" + m + \" n=\" + n )\n",
    "  mat.rows.foreach( println(_) )  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "IndexedRowMatrix 和 RowMatrix 的不同在于每行多了long类型的index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m=8 n=3\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix, RowMatrix}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "val ad_tags = Seq(\n",
    "    IndexedRow(1, Vectors.dense( 3, 1,1) ),\n",
    "    IndexedRow(2, Vectors.dense( 5, 2,1) ),\n",
    "    IndexedRow(3, Vectors.dense( 6, 3,1) ),\n",
    "    IndexedRow(4, Vectors.dense( 7, 5,1) ),\n",
    "    IndexedRow(5, Vectors.dense( 8, 6,1) ),\n",
    "    IndexedRow(6, Vectors.dense( 6, 8,1) ),\n",
    "    IndexedRow(7, Vectors.dense( 8, 9,1) )\n",
    "  )\n",
    "\n",
    "  val rows: RDD[IndexedRow] = sc.parallelize( ad_tags )\n",
    "\n",
    "  // Create an IndexedRowMatrix from an RDD[IndexedRow].\n",
    "  val mat: IndexedRowMatrix = new IndexedRowMatrix(rows)\n",
    "\n",
    "  // Get its size.\n",
    "  val m = mat.numRows()\n",
    "  val n = mat.numCols()\n",
    "  println( \"m=\" + m + \" n=\" + n )\n",
    "  mat.rows.foreach(println(_))\n",
    "  // Drop its row indices.\n",
    "  val rowMat: RowMatrix = mat.toRowMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CoordinateMatrix \n",
    "CoordinateMatrix 通常被用于较大的稀疏矩阵，每个元素的值是一个MatrixEntry(row,column,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m=6 n=9\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val ad_tags = Seq(\n",
    "    MatrixEntry(2,3,4.0),\n",
    "    MatrixEntry(2,8,1.0),\n",
    "    MatrixEntry(3,2,4.0),\n",
    "    MatrixEntry(5,1,3.0),\n",
    "    MatrixEntry(5,4,7.0),\n",
    "    MatrixEntry(5,7,2.0)\n",
    "  )\n",
    "\n",
    "  val entries: RDD[MatrixEntry] = sc.parallelize( ad_tags ) // an RDD of matrix entries\n",
    "  // Create a CoordinateMatrix from an RDD[MatrixEntry].\n",
    "  val mat: CoordinateMatrix = new CoordinateMatrix(entries)\n",
    "  mat.entries.foreach(println(_))\n",
    "  // Get its size.\n",
    "  val m = mat.numRows()\n",
    "  val n = mat.numCols()\n",
    "  println( \"m=\" + m + \" n=\" + n )\n",
    "  // Convert it to an IndexRowMatrix whose rows are sparse vectors.\n",
    "  val indexedRowMatrix = mat.toIndexedRowMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BlockMatrix\n",
    "BlockMatrix is a distributed matrix backed by an RDD of MatrixBlocks, where a MatrixBlock is a tuple of ((Int, Int), Matrix), where the (Int, Int) is the index of the block, and Matrix is the sub-matrix at the given index with size rowsPerBlock x colsPerBlock. BlockMatrix supports methods such as add and multiply with another BlockMatrix. BlockMatrix also has a helper function validate which can be used to check whether the BlockMatrix is set up properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.distributed.{BlockMatrix, CoordinateMatrix, MatrixEntry}\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val ad_tags = Seq(\n",
    "    MatrixEntry(2,3,4.0),\n",
    "    MatrixEntry(2,8,1.0),\n",
    "    MatrixEntry(3,2,4.0),\n",
    "    MatrixEntry(5,1,3.0),\n",
    "    MatrixEntry(5,4,7.0),\n",
    "    MatrixEntry(5,7,2.0)\n",
    "  )\n",
    "\n",
    "  val entries: RDD[MatrixEntry] = sc.parallelize( ad_tags ) // an RDD of matrix entries\n",
    "  // Create a CoordinateMatrix from an RDD[MatrixEntry].\n",
    "  val coordMat: CoordinateMatrix = new CoordinateMatrix(entries)\n",
    "  // Transform the CoordinateMatrix to a BlockMatrix\n",
    "  val matA: BlockMatrix = coordMat.toBlockMatrix().cache()\n",
    "  matA.blocks.foreach(println(_))\n",
    "  // Validate whether the BlockMatrix is set up properly. Throws an Exception when it is not valid.\n",
    "  // Nothing happens if it is valid.\n",
    "  matA.validate()\n",
    "  \n",
    "  // Calculate A^T A.\n",
    "  val ata = matA.transpose.multiply(matA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.5.2 (Scala 2.10)",
   "language": "",
   "name": "spark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
