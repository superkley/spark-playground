{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics\n",
    "colStats函数用户汇总统计RDD[Vector]的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0,20.0,200.0]\n",
      "[1.0,100.0,10000.0]\n",
      "[3.0,3.0,3.0]\n",
      "[1.0,10.0,100.0]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\n",
    "\n",
    "val observations = sc.parallelize(\n",
    "  Seq(\n",
    "    Vectors.dense(1.0, 10.0, 100.0),\n",
    "    Vectors.dense(2.0, 20.0, 200.0),\n",
    "    Vectors.dense(3.0, 30.0, 300.0)\n",
    "  )\n",
    ")\n",
    "\n",
    "// Compute column summary statistics.\n",
    "val summary: MultivariateStatisticalSummary = Statistics.colStats(observations)\n",
    "println(summary.mean)  // a dense vector containing the mean value for each column\n",
    "println(summary.variance)  // column-wise variance\n",
    "println(summary.numNonzeros)  // number of nonzeros in each column\n",
    "println(summary.min) //min value for each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation is: 0.8500286768773001\n",
      "1.0                 0.978883465889473   0.9903895695275671  \n",
      "0.978883465889473   1.0                 0.9977483233986101  \n",
      "0.9903895695275671  0.9977483233986101  1.0                 \n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.stat.Statistics\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val seriesX: RDD[Double] = sc.parallelize(Array(1, 2, 3, 3, 5))  // a series\n",
    "// must have the same number of partitions and cardinality as seriesX\n",
    "val seriesY: RDD[Double] = sc.parallelize(Array(11, 22, 33, 33, 555))\n",
    "\n",
    "// compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method. If a\n",
    "// method is not specified, Pearson's method will be used by default.\n",
    "val correlation: Double = Statistics.corr(seriesX, seriesY, \"pearson\")\n",
    "println(s\"Correlation is: $correlation\")\n",
    "\n",
    "val data: RDD[Vector] = sc.parallelize(\n",
    "  Seq(\n",
    "    Vectors.dense(1.0, 10.0, 100.0),\n",
    "    Vectors.dense(2.0, 20.0, 200.0),\n",
    "    Vectors.dense(5.0, 33.0, 366.0))\n",
    ")  // note that each Vector is a row and not a column\n",
    "\n",
    "// calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method\n",
    "// If a method is not specified, Pearson's method will be used by default.\n",
    "val correlMatrix: Matrix = Statistics.corr(data, \"pearson\")\n",
    "println(correlMatrix.toString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// an RDD[(K, V)] of any key value pairs\n",
    "val data = sc.parallelize(\n",
    "  Seq((1, 'a'), (1, 'b'), (2, 'c'), (2, 'd'), (2, 'e'), (3, 'f')))\n",
    "\n",
    "// specify the exact fraction desired from each key\n",
    "val fractions = Map(1 -> 0.1, 2 -> 0.6, 3 -> 0.3)\n",
    "\n",
    "// Get an approximate sample from each stratum\n",
    "val approxSample = data.sampleByKey(withReplacement = false, fractions = fractions)\n",
    "approxSample.foreach(println(_))\n",
    "// Get an exact sample from each stratum\n",
    "val exactSample = data.sampleByKeyExact(withReplacement = false, fractions = fractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 4 \n",
      "statistic = 0.12499999999999999 \n",
      "pValue = 0.998126379239318 \n",
      "No presumption against null hypothesis: observed follows the same distribution as expected..\n",
      "\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 2 \n",
      "statistic = 0.14141414141414144 \n",
      "pValue = 0.931734784568187 \n",
      "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
      "\n",
      "Column 1:\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 1 \n",
      "statistic = 3.0000000000000004 \n",
      "pValue = 0.08326451666354884 \n",
      "Low presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
      "Column 2:\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 1 \n",
      "statistic = 0.75 \n",
      "pValue = 0.3864762307712326 \n",
      "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
      "Column 3:\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 2 \n",
      "statistic = 3.0 \n",
      "pValue = 0.22313016014843035 \n",
      "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.stat.Statistics\n",
    "import org.apache.spark.mllib.stat.test.ChiSqTestResult\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "// a vector composed of the frequencies of events\n",
    "val vec: Vector = Vectors.dense(0.1, 0.15, 0.2, 0.3, 0.25)\n",
    "\n",
    "// compute the goodness of fit. If a second vector to test against is not supplied\n",
    "// as a parameter, the test runs against a uniform distribution.\n",
    "val goodnessOfFitTestResult = Statistics.chiSqTest(vec)\n",
    "// summary of the test including the p-value, degrees of freedom, test statistic, the method\n",
    "// used, and the null hypothesis.\n",
    "println(s\"$goodnessOfFitTestResult\\n\")\n",
    "\n",
    "// a contingency matrix. Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\n",
    "val mat: Matrix = Matrices.dense(3, 2, Array(1.0, 3.0, 5.0, 2.0, 4.0, 6.0))\n",
    "\n",
    "// conduct Pearson's independence test on the input contingency matrix\n",
    "val independenceTestResult = Statistics.chiSqTest(mat)\n",
    "// summary of the test including the p-value, degrees of freedom\n",
    "println(s\"$independenceTestResult\\n\")\n",
    "\n",
    "val obs: RDD[LabeledPoint] =\n",
    "  sc.parallelize(\n",
    "    Seq(\n",
    "      LabeledPoint(1.0, Vectors.dense(1.0, 0.0, 3.0)),\n",
    "      LabeledPoint(1.0, Vectors.dense(1.0, 2.0, 0.0)),\n",
    "      LabeledPoint(-1.0, Vectors.dense(-1.0, 0.0, -0.5)\n",
    "      )\n",
    "    )\n",
    "  ) // (feature, label) pairs.\n",
    "\n",
    "// The contingency table is constructed from the raw (feature, label) pairs and used to conduct\n",
    "// the independence test. Returns an array containing the ChiSquaredTestResult for every feature\n",
    "// against the label.\n",
    "val featureTestResults: Array[ChiSqTestResult] = Statistics.chiSqTest(obs)\n",
    "featureTestResults.zipWithIndex.foreach { case (k, v) =>\n",
    "  println(\"Column \" + (v + 1).toString + \":\")\n",
    "  println(k)\n",
    "}  // summary of the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolmogorov-Smirnov test summary:\n",
      "degrees of freedom = 0 \n",
      "statistic = 0.539827837277029 \n",
      "pValue = 0.06821463111921133 \n",
      "Low presumption against null hypothesis: Sample follows theoretical distribution.\n",
      "Kolmogorov-Smirnov test summary:\n",
      "degrees of freedom = 0 \n",
      "statistic = 0.95 \n",
      "pValue = 6.249999999763389E-7 \n",
      "Very strong presumption against null hypothesis: Sample follows theoretical distribution.\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.stat.Statistics\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val data: RDD[Double] = sc.parallelize(Seq(0.1, 0.15, 0.2, 0.3, 0.25))  // an RDD of sample data\n",
    "\n",
    "// run a KS test for the sample versus a standard normal distribution\n",
    "val testResult = Statistics.kolmogorovSmirnovTest(data, \"norm\", 0, 1)\n",
    "// summary of the test including the p-value, test statistic, and null hypothesis if our p-value\n",
    "// indicates significance, we can reject the null hypothesis.\n",
    "println(testResult)\n",
    "println()\n",
    "\n",
    "// perform a KS test using a cumulative distribution function of our making\n",
    "val myCDF = Map(0.1 -> 0.2, 0.15 -> 0.6, 0.2 -> 0.05, 0.3 -> 0.05, 0.25 -> 0.1)\n",
    "val testResult2 = Statistics.kolmogorovSmirnovTest(data, myCDF)\n",
    "println(testResult2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Random data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.random.RandomRDDs._\n",
    "// Generate a random double RDD that contains 1 million i.i.d. values drawn from the\n",
    "// standard normal distribution `N(0, 1)`, evenly distributed in 10 partitions.\n",
    "val u = normalRDD(sc, 1000000L, 10)\n",
    "// Apply a transform to get a random double RDD following `N(1, 4)`.\n",
    "val v = u.map(x => 1.0 + 2.0 * x)\n",
    "println(u)\n",
    "println(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel density estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.stat.KernelDensity\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "// an RDD of sample data\n",
    "val data: RDD[Double] = sc.parallelize(Seq(1, 1, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 9))\n",
    "\n",
    "// Construct the density estimator with the sample data and a standard deviation\n",
    "// for the Gaussian kernels\n",
    "val kd = new KernelDensity().setSample(data).setBandwidth(3.0)\n",
    "\n",
    "// Find density estimates for the given values\n",
    "val densities = kd.estimate(Array(-1.0, 2.0, 5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.5.2 (Scala 2.10)",
   "language": "",
   "name": "spark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
