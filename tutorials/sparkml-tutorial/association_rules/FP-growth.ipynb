{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP-growth\n",
    "FP-Growth(频繁模式增长)算法是韩家炜老师在2000年提出的关联分析算法，它采取如下分治策略：将提供频繁项集的数据库压缩到一棵频繁模式树（FP-Tree），但仍保留项集关联信息；该算法和Apriori算法最大的不同有两点：第一，不产生候选集，第二，只需要两次遍历数据库，大大提高了效率。\n",
    "\n",
    "## 演示—构造FP-树\n",
    "***（1）事务数据库建立 ***  \n",
    "原始事务数据库如下：\n",
    "<table border=\"1\" width=\"70%\" style=\"margin-left:0px\">\n",
    "<thead>\n",
    "    <tr>\n",
    "      <th>Tid</th>\n",
    "      <th>items</th>\n",
    "    </tr>\n",
    "</thead>\n",
    "<tr> <td> 1 </td><td> I1,I2,I5 </td> </tr>\n",
    "<tr> <td> 2 </td><td> I2,I4 </td> </tr>\n",
    "<tr> <td> 3 </td><td> I2,I3 </td> </tr>\n",
    "<tr> <td> 4 </td><td> I1,I2,I4 </td> </tr>\n",
    "<tr> <td> 5 </td><td> I1,I3 </td> </tr>\n",
    "<tr> <td> 6 </td><td> I2,I3 </td> </tr>\n",
    "<tr> <td> 7 </td><td>I1,I3 </td> </tr>\n",
    "<tr> <td> 8 </td><td> I1,I2,I3,I5 </td> </tr>\n",
    "<tr> <td> 9 </td><td> I1,I2,I3 </td> </tr>\n",
    "</table>\n",
    "\n",
    "扫描事务数据库得到频繁1-项目集F。  \n",
    "I1:6 I2:7 I3:6  I4:2  I5:2    \n",
    "定义minsup=20%，即最小支持度为2，重新排列F。\n",
    "I2:7  I1:6  I3:6  I4:2  I5:2  \n",
    "重新调整事务数据库。\n",
    "<table border=\"1\" width=\"70%\" style=\"margin-left:0px\">\n",
    "<thead>\n",
    "    <tr>\n",
    "      <th>Tid</th>\n",
    "      <th>items</th>\n",
    "    </tr>\n",
    "</thead>\n",
    "<tr> <td> 1 </td><td> I2, I1,I5 </td> </tr>\n",
    "<tr> <td> 2 </td><td> I2,I4 </td> </tr>\n",
    "<tr> <td> 3 </td><td> I2,I3 </td> </tr>\n",
    "<tr> <td> 4 </td><td> I2, I1,I4 </td> </tr>\n",
    "<tr> <td> 5 </td><td> I1,I3 </td> </tr>\n",
    "<tr> <td> 6 </td><td> I2,I3 </td> </tr>\n",
    "<tr> <td> 7 </td><td> I1,I3 </td> </tr>\n",
    "<tr> <td> 8 </td><td> I2, I1,I3,I5 </td> </tr>\n",
    "<tr> <td> 9 </td><td> I2, I1,I3 </td> </tr>\n",
    "</table>\n",
    "\n",
    "*** （2）创建根结点和频繁项目表 ***\n",
    "<img src=\"./images/fp/1.png\" width = \"500px\" style=\"margin-left:0px\"/>\n",
    "***（3）加入第一个事务(I2,I1,I5) ***\n",
    "<img src=\"./images/fp/2.png\" width = \"500px\" style=\"margin-left:0px\"/>\n",
    "*** （4）加入第二个事务(I2,I4) ***\n",
    "<img src=\"./images/fp/3.png\" width = \"500px\" style=\"margin-left:0px\"/>\n",
    "*** （5）加入第三个事务(I2,I3) ***\n",
    "<img src=\"./images/fp/4.png\" width = \"500px\" style=\"margin-left:0px\"/>\n",
    "以此类推加入第5、6、7、8、9个事务。\n",
    "*** （6）加入第九个事务(I2,I1,I3) ***\n",
    "<img src=\"./images/fp/5.png\" width = \"500px\" style=\"margin-left:0px\"/>\n",
    "\n",
    "## 演示—FP-树挖掘\n",
    "FP-树建好后，就可以进行频繁项集的挖掘，挖掘算法称为FpGrowth（Frequent Pattern Growth）算法，挖掘从表头header的最后一个项开始，以此类推。本文以I5、I3为例进行挖掘。  \n",
    "*** （1）挖掘I5： ***  \n",
    "对于I5，得到条件模式基：<(I2,I1:1)>、<I2,I1,I3:1>  \n",
    "构造条件FP-tree：\n",
    "<img src=\"./images/fp/6.png\" width = \"500px\" style=\"margin-left:0px\"/>  \n",
    "得到I5频繁项集：{{I2,I5:2},{I1,I5:2},{I2,I1,I5:2}}  \n",
    "I4、I1的挖掘与I5类似，条件FP-树都是单路径。  \n",
    "***（2）挖掘I3：***  \n",
    "I5的情况是比较简单的，因为I5对应的条件FP-树是单路径的，I3稍微复杂一点。I3的条件模式基是(I2 I1:2), (I2:2), (I1:2)，生成的条件FP-树如下图：\n",
    "<img src=\"./images/fp/7.png\" width = \"500px\" style=\"margin-left:0px\"/>  \n",
    "I3的条件FP-树仍然是一个多路径树，首先把模式后缀I3和条件FP-树中的项头表中的每一项取并集，得到一组模式{I2 I3:4, I1 I3:4}，但是这一组模式不是后缀为I3的所有模式。还需要递归调用FP-growth，模式后缀为{I1，I3}，{I1，I3}的条件模式基为{I2：2}，其生成的条件FP-树如下图所示。  \n",
    "<img src=\"./images/fp/8.png\" width = \"500px\" style=\"margin-left:0px\"/>  \n",
    "在FP_growth中把I2和模式后缀{I1，I3}取并得到模式{I1 I2 I3：2}。\n",
    "\n",
    "理论上还应该计算一下模式后缀为{I2，I3}的模式集，但是{I2，I3}的条件模式基为空，递归调用结束。最终模式后缀I3的支持度>2的所有模式为：{ I2 I3:4, I1 I3:4, I1 I2 I3:2}。 \n",
    "\n",
    "## Spark MLlib 中的 FP-growth 算法参数：\n",
    "* minSupport: 最小支持度\n",
    "* numPartitions: 分布式运行情况下分区数  \n",
    "\n",
    "## 例子（Examples）\n",
    "FPGrowth 实现了 FP-growth 算法。该算法采用的是 RDD 事务每个 RDD 都是 Array 类型，运行该事务就返回 FPGrowthModel 用该模型存储频繁项集和它们的频率。下面的示例演示如何从事务中挖掘频繁项集和关联规则。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[z], 5\n",
      "[x], 4\n",
      "[x,z], 3\n",
      "[y], 3\n",
      "[y,x], 3\n",
      "[y,x,z], 3\n",
      "[y,z], 3\n",
      "[r], 3\n",
      "[r,x], 2\n",
      "[r,z], 2\n",
      "[s], 3\n",
      "[s,y], 2\n",
      "[s,y,x], 2\n",
      "[s,y,x,z], 2\n",
      "[s,y,z], 2\n",
      "[s,x], 3\n",
      "[s,x,z], 2\n",
      "[s,z], 2\n",
      "[t], 3\n",
      "[t,y], 3\n",
      "[t,y,x], 3\n",
      "[t,y,x,z], 3\n",
      "[t,y,z], 3\n",
      "[t,s], 2\n",
      "[t,s,y], 2\n",
      "[t,s,y,x], 2\n",
      "[t,s,y,x,z], 2\n",
      "[t,s,y,z], 2\n",
      "[t,s,x], 2\n",
      "[t,s,x,z], 2\n",
      "[t,s,z], 2\n",
      "[t,x], 3\n",
      "[t,x,z], 3\n",
      "[t,z], 3\n",
      "[p], 2\n",
      "[p,r], 2\n",
      "[p,r,z], 2\n",
      "[p,z], 2\n",
      "[q], 2\n",
      "[q,y], 2\n",
      "[q,y,x], 2\n",
      "[q,y,x,z], 2\n",
      "[q,y,z], 2\n",
      "[q,t], 2\n",
      "[q,t,y], 2\n",
      "[q,t,y,x], 2\n",
      "[q,t,y,x,z], 2\n",
      "[q,t,y,z], 2\n",
      "[q,t,x], 2\n",
      "[q,t,x,z], 2\n",
      "[q,t,z], 2\n",
      "[q,x], 2\n",
      "[q,x,z], 2\n",
      "[q,z], 2\n",
      "[t,s,y] => [x], 1.0\n",
      "[t,s,y] => [z], 1.0\n",
      "[y,x,z] => [t], 1.0\n",
      "[y] => [x], 1.0\n",
      "[y] => [z], 1.0\n",
      "[y] => [t], 1.0\n",
      "[p] => [r], 1.0\n",
      "[p] => [z], 1.0\n",
      "[q,t,z] => [y], 1.0\n",
      "[q,t,z] => [x], 1.0\n",
      "[q,y] => [x], 1.0\n",
      "[q,y] => [z], 1.0\n",
      "[q,y] => [t], 1.0\n",
      "[t,s,x] => [y], 1.0\n",
      "[t,s,x] => [z], 1.0\n",
      "[q,t,y,z] => [x], 1.0\n",
      "[q,t,x,z] => [y], 1.0\n",
      "[q,x] => [y], 1.0\n",
      "[q,x] => [t], 1.0\n",
      "[q,x] => [z], 1.0\n",
      "[t,x,z] => [y], 1.0\n",
      "[x,z] => [y], 1.0\n",
      "[x,z] => [t], 1.0\n",
      "[p,z] => [r], 1.0\n",
      "[t] => [y], 1.0\n",
      "[t] => [x], 1.0\n",
      "[t] => [z], 1.0\n",
      "[y,z] => [x], 1.0\n",
      "[y,z] => [t], 1.0\n",
      "[p,r] => [z], 1.0\n",
      "[t,s] => [y], 1.0\n",
      "[t,s] => [x], 1.0\n",
      "[t,s] => [z], 1.0\n",
      "[q,z] => [y], 1.0\n",
      "[q,z] => [t], 1.0\n",
      "[q,z] => [x], 1.0\n",
      "[q,y,z] => [x], 1.0\n",
      "[q,y,z] => [t], 1.0\n",
      "[y,x] => [z], 1.0\n",
      "[y,x] => [t], 1.0\n",
      "[q,x,z] => [y], 1.0\n",
      "[q,x,z] => [t], 1.0\n",
      "[t,y,z] => [x], 1.0\n",
      "[q,y,x] => [z], 1.0\n",
      "[q,y,x] => [t], 1.0\n",
      "[q,t,y,x] => [z], 1.0\n",
      "[t,s,x,z] => [y], 1.0\n",
      "[s,y,x] => [z], 1.0\n",
      "[s,y,x] => [t], 1.0\n",
      "[s,x,z] => [y], 1.0\n",
      "[s,x,z] => [t], 1.0\n",
      "[q,y,x,z] => [t], 1.0\n",
      "[s,y] => [x], 1.0\n",
      "[s,y] => [z], 1.0\n",
      "[s,y] => [t], 1.0\n",
      "[q,t,y] => [x], 1.0\n",
      "[q,t,y] => [z], 1.0\n",
      "[t,y] => [x], 1.0\n",
      "[t,y] => [z], 1.0\n",
      "[t,z] => [y], 1.0\n",
      "[t,z] => [x], 1.0\n",
      "[t,s,y,x] => [z], 1.0\n",
      "[t,y,x] => [z], 1.0\n",
      "[q,t] => [y], 1.0\n",
      "[q,t] => [x], 1.0\n",
      "[q,t] => [z], 1.0\n",
      "[q] => [y], 1.0\n",
      "[q] => [t], 1.0\n",
      "[q] => [x], 1.0\n",
      "[q] => [z], 1.0\n",
      "[t,s,z] => [y], 1.0\n",
      "[t,s,z] => [x], 1.0\n",
      "[t,x] => [y], 1.0\n",
      "[t,x] => [z], 1.0\n",
      "[s,z] => [y], 1.0\n",
      "[s,z] => [x], 1.0\n",
      "[s,z] => [t], 1.0\n",
      "[s,y,x,z] => [t], 1.0\n",
      "[s] => [x], 1.0\n",
      "[t,s,y,z] => [x], 1.0\n",
      "[s,y,z] => [x], 1.0\n",
      "[s,y,z] => [t], 1.0\n",
      "[q,t,x] => [y], 1.0\n",
      "[q,t,x] => [z], 1.0\n",
      "[r,z] => [p], 1.0\n"
     ]
    }
   ],
   "source": [
    "val PATH = \"file:///Users/lzz/work/SparkML/\"\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.fpm.FPGrowth\n",
    "\n",
    "val data = sc.textFile(PATH+\"data/mllib/sample_fpgrowth.txt\")\n",
    "\n",
    "val transactions: RDD[Array[String]] = data.map(s => s.trim.split(' '))\n",
    "\n",
    "val fpg = new FPGrowth().setMinSupport(0.2).setNumPartitions(10)\n",
    "val model = fpg.run(transactions)\n",
    "  \n",
    "model.freqItemsets.collect().foreach { itemset =>\n",
    "  println(itemset.items.mkString(\"[\", \",\", \"]\") + \", \" + itemset.freq)\n",
    "}\n",
    "\n",
    "val minConfidence = 0.8\n",
    "model.generateAssociationRules(minConfidence).collect().foreach { rule =>\n",
    "  println(\n",
    "    rule.antecedent.mkString(\"[\", \",\", \"]\")\n",
    "      + \" => \" + rule.consequent .mkString(\"[\", \",\", \"]\")\n",
    "      + \", \" + rule.confidence)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.5.2 (Scala 2.10)",
   "language": "",
   "name": "spark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
