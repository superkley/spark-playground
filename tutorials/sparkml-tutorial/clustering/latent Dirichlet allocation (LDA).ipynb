{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet allocation (LDA)\n",
    "Latent Dirichlet allocation (LDA) is a topic model which infers topics from a collection of text documents. LDA can be thought of as a clustering algorithm as follows:  \n",
    "*  Topics correspond to cluster centers, and documents correspond to examples (rows) in a dataset.\n",
    "*  Topics and documents both exist in a feature space, where feature vectors are vectors of word counts (bag of words).\n",
    "*  Rather than estimating a clustering using a traditional distance, LDA uses a function based on a statistical model of how text documents are generated.  \n",
    "\n",
    "LDA supports different inference algorithms via setOptimizer function. EMLDAOptimizer learns clustering using expectation-maximization on the likelihood function and yields comprehensive results, while OnlineLDAOptimizer uses iterative mini-batch sampling for online variational inference and is generally memory friendly.  \n",
    "\n",
    "LDA takes in a collection of documents as vectors of word counts and the following parameters (set using the builder pattern):\n",
    "\n",
    "* k: Number of topics (i.e., cluster centers)\n",
    "* optimizer: Optimizer to use for learning the LDA model, either EMLDAOptimizer or OnlineLDAOptimizer\n",
    "* docConcentration: Dirichlet parameter for prior over documents’ distributions over topics. Larger values encourage smoother inferred distributions.\n",
    "* topicConcentration: Dirichlet parameter for prior over topics’ distributions over terms (words). Larger values encourage smoother inferred distributions.\n",
    "* maxIterations: Limit on the number of iterations.\n",
    "* checkpointInterval: If using checkpointing (set in the Spark configuration), this parameter specifies the frequency with which checkpoints will be created. If maxIterations is large, using checkpointing can help reduce shuffle file sizes on disk and help with failure recovery.\n",
    "\n",
    "All of MLlib’s LDA models support:\n",
    "\n",
    "* describeTopics: Returns topics as arrays of most important terms and term weights\n",
    "* topicsMatrix: Returns a vocabSize by k matrix where each column is a topic\n",
    "\n",
    "Note: LDA is still an experimental feature under active development. As a result, certain features are only available in one of the two optimizers / models generated by the optimizer. Currently, a distributed model can be converted into a local model, but not vice-versa.  \n",
    "\n",
    "The following discussion will describe each optimizer/model pair separately.  \n",
    "\n",
    "### Expectation Maximization  \n",
    "Implemented in EMLDAOptimizer and DistributedLDAModel.  \n",
    "For the parameters provided to LDA:  \n",
    "* docConcentration: Only symmetric priors are supported, so all values in the provided k-dimensional vector must be identical. All values must also be >1.0>1.0. Providing Vector(-1) results in default behavior (uniform k dimensional vector with value (50/k)+1(50/k)+1\n",
    "* topicConcentration: Only symmetric priors supported. Values must be >1.0>1.0. Providing -1 results in defaulting to a value of 0.1+10.1+1.\n",
    "* maxIterations: The maximum number of EM iterations.  \n",
    "\n",
    "EMLDAOptimizer produces a DistributedLDAModel, which stores not only the inferred topics but also the full training corpus and topic distributions for each document in the training corpus. A DistributedLDAModel supports:  \n",
    "\n",
    "* topTopicsPerDocument: The top topics and their weights for each document in the training corpus\n",
    "* topDocumentsPerTopic: The top documents for each topic and the corresponding weight of the topic in the documents.\n",
    "* logPrior: log probability of the estimated topics and document-topic distributions given the hyperparameters docConcentration and topicConcentration\n",
    "* logLikelihood: log likelihood of the training corpus, given the inferred topics and document-topic distributions  \n",
    "\n",
    "### Online Variational Bayes\n",
    "Implemented in OnlineLDAOptimizer and LocalLDAModel.  \n",
    "For the parameters provided to LDA:  \n",
    "* docConcentration: Asymmetric priors can be used by passing in a vector with values equal to the Dirichlet parameter in each of the k dimensions. Values should be >=0>=0. Providing Vector(-1) results in default behavior (uniform k dimensional vector with value (1.0/k)(1.0/k))\n",
    "* topicConcentration: Only symmetric priors supported. Values must be >=0>=0. Providing -1 results in defaulting to a value of (1.0/k)(1.0/k).\n",
    "* maxIterations: Maximum number of minibatches to submit.  \n",
    "\n",
    "In addition, OnlineLDAOptimizer accepts the following parameters:  \n",
    "\n",
    "* miniBatchFraction: Fraction of corpus sampled and used at each iteration\n",
    "* optimizeDocConcentration: If set to true, performs maximum-likelihood estimation of the hyperparameter docConcentration (aka alpha) after each minibatch and sets the optimized docConcentration in the returned LocalLDAModel\n",
    "* tau0 and kappa: Used for learning-rate decay, which is computed by $(\\tau_0 + iter)^{-\\kappa}$ where iteriter is the current number of iterations.  \n",
    "\n",
    "OnlineLDAOptimizer produces a LocalLDAModel, which only stores the inferred topics. A LocalLDAModel supports:  \n",
    "\n",
    "* logLikelihood(documents): Calculates a lower bound on the provided documents given the inferred topics.\n",
    "* logPerplexity(documents): Calculates an upper bound on the perplexity of the provided documents given the inferred topics.  \n",
    "\n",
    "### Examples\n",
    "In the following example, we load word count vectors representing a corpus of documents. We then use LDA to infer three topics from the documents. The number of desired clusters is passed to the algorithm. We then output the topics, represented as probability distributions over words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned topics (as distributions over vocab of 11 words):\n",
      "Topic 0: 11.082131945394131 13.096273230955001 1.7828503882078757 5.335951336627549 13.554427641829196 5.244268769875691 6.647561701256702 3.432112549375341 2.9725230849045166 8.782699890300785 14.155634058335988\n",
      "Topic 1: 7.649720574244201 4.270419292006031 2.6589606318530206 32.830920089898314 4.0704441118472054 5.322411996044803 8.22113549731758 1.5225070821672357 2.241963856600081 6.9539310872312585 10.94090835710875\n",
      "Topic 2: 7.268147480361669 11.633307477038969 7.5581889799391035 1.8331285734741423 7.375128246323598 11.433319234079507 16.131302801425722 5.0453803684574225 2.785513058495403 8.263369022467955 7.903457584555262\n"
     ]
    }
   ],
   "source": [
    "val PATH = \"file:///Users/lzz/work/SparkML/\"\n",
    "\n",
    "import org.apache.spark.mllib.clustering.{LDA, DistributedLDAModel}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "// Load and parse the data\n",
    "val data = sc.textFile( PATH + \"data/mllib/sample_lda_data.txt\")\n",
    "val parsedData = data.map(s => Vectors.dense(s.trim.split(' ').map(_.toDouble)))\n",
    "// Index documents with unique IDs\n",
    "val corpus = parsedData.zipWithIndex.map(_.swap).cache()\n",
    "\n",
    "// Cluster the documents into three topics using LDA\n",
    "val ldaModel = new LDA().setK(3).run(corpus)\n",
    "\n",
    "// Output topics. Each is a distribution over words (matching word count vectors)\n",
    "println(\"Learned topics (as distributions over vocab of \" + ldaModel.vocabSize + \" words):\")\n",
    "val topics = ldaModel.topicsMatrix\n",
    "for (topic <- Range(0, 3)) {\n",
    "  print(\"Topic \" + topic + \":\")\n",
    "  for (word <- Range(0, ldaModel.vocabSize)) { print(\" \" + topics(word, topic)); }\n",
    "  println()\n",
    "}\n",
    "\n",
    "// Save and load model.\n",
    "ldaModel.save(sc, \"myLDAModel\")\n",
    "val sameModel = DistributedLDAModel.load(sc, \"myLDAModel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.5.2 (Scala 2.10)",
   "language": "",
   "name": "spark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
