# 交替最小二乘法
ALS（alternating least squares）

例如：
将用户(user)对商品(item)的评分矩阵分解为两个矩阵：一个是用户对商品隐含特征的偏好矩阵，另一个是商品所包含的隐含特征的矩阵
ALS旨在找到两个低维矩阵X(m×k)和矩阵Y(n×k)，来近似逼近R(m×n)

<img src="./formulary/als/1.png" width='150px' />

为了找到使低秩矩阵X和Y尽可能地逼近R，需要最小化下面的平方误差损失函数：

<img src="./formulary/als/2.png" width='200px' />

其中xu(1×k)表示示用户u的偏好的隐含特征向量，yi(1×k)表示商品i包含的隐含特征向量, rui表示用户u对商品i的评分, 向量xu和yi的内积xuTyi是用户u对商品i评分的近似
由于变量xu和yi耦合到一起，这个问题并不好求解，所以我们引入了ALS，也就是说我们可以先固定Y（例如随机初始化X），然后利用公式（2）先求解X，然后固定X，再求解Y，如此交替往复直至收敛，即所谓的交替最小二乘法求解法。

为了让模型更稳定损失函数一般需要加入正则化项来避免过拟合等问题，我们使用L2正则化（l0范式表示向量中非零元素对个数，l1表示向量中元素对绝对值之和，比如有用户A和B它们分别定了十次外卖，A给了2次评论B给了8次评论，那么我们会觉得B用户对数据更能反馈商品对质量。假设A给了两次（1，5）B也评论了两次（3，3），虽然l1范式计算的结果都是6，但是A的数据比较情绪化就是不稳定，B用户比较认真去评论商品，所以我们可以计算向量的平方和来比较这时A＝26 B＝18，这就是L2范式）

<img src="./formulary/als/3.png" width='300px' />

其中λ是正则化项的系数（也就是惩罚的力度，可以根据具体数据进行设置）

具体步骤
先固定Y,  将损失函数L(X,Y)对xu求偏导，并令导数=0
<img src="./formulary/als/4.png" width='220px' />

然后再固定X

<img src="./formulary/als/5.png" width='220px' />

其中ru(1×n)是R的第u行,ri(1×m)是R的第i列， I是k×k的单位矩阵。

迭代步骤：首先随机初始化Y，利用公式(3)更新得到X,  然后利用公式(4)更新Y,  直到均方根误差变RMSE化很小或者到达最大迭代次数。

<img src="./formulary/als/6.png" width='150px' />

上文提到的模型适用于解决有明确评分矩阵的应用场景，然而很多情况下，用户没有明确反馈对商品的偏好，也就是没有直接打分，我们只能通过用户的某些行为来推断他对商品的偏好。比如，在电视节目推荐的问题中，对电视节目收看的次数或者时长，这时我们可以推测次数越多，看得时间越长，用户的偏好程度越高，但是对于没有收看的节目，可能是由于用户不知道有该节目，或者没有途径获取该节目，我们不能确定的推测用户不喜欢该节目。ALS-WR通过置信度权重来解决这些问题：对于更确信用户偏好的项赋以较大的权重，对于没有反馈的项，赋以较小的权重。ALS-WR模型的形式化说明如下：

ALS-WR的目标函数：

<img src="./formulary/als/7.png" width='400px' />

其中α是置信度系数。

求解方式还是最小二乘法：

<img src="./formulary/als/8.png" width='250px' />

其中Cu是n×n的对角矩阵，Ci是m×m的对角矩阵；Cuii  = cui,  Ciii  = cii。



参考：
http://www.cnblogs.com/hxsyl/p/5032691.html
