{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Kaggle StumbleUpon evergreen classification\n",
    "It aim to judge that a web page would be evergreen,short lived or cease\n",
    "\n",
    " - [data downloaded url](https://www.kaggle.com/c/stumbleupon/data)\n",
    " - [more information about this competition](https://www.kaggle.com/c/stumbleupon) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉train中的header信息\n",
    "!sed 1d ../data/evergreen_classification/train.tsv > ../data/evergreen_classification/train_noheader.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'\"http://www.bloomberg.com/news/2010-12-23/ibm-predicts-holographic-calls-air-breathing-batteries-by-2015.html\"',\n",
       "  u'\"4042\"',\n",
       "  u'\"{\"\"title\"\":\"\"IBM Sees Holographic Calls Air Breathing Batteries ibm sees holographic calls, air-breathing batteries\"\",\"\"body\"\":\"\"A sign stands outside the International Business Machines Corp IBM Almaden Research Center campus in San Jose California Photographer Tony Avelar Bloomberg Buildings stand at the International Business Machines Corp IBM Almaden Research Center campus in the Santa Teresa Hills of San Jose California Photographer Tony Avelar Bloomberg By 2015 your mobile phone will project a 3 D image of anyone who calls and your laptop will be powered by kinetic energy At least that s what International Business Machines Corp sees in its crystal ball The predictions are part of an annual tradition for the Armonk New York based company which surveys its 3 000 researchers to find five ideas expected to take root in the next five years IBM the world s largest provider of computer services looks to Silicon Valley for input gleaning many ideas from its Almaden research center in San Jose California Holographic conversations projected from mobile phones lead this year s list The predictions also include air breathing batteries computer programs that can tell when and where traffic jams will take place environmental information generated by sensors in cars and phones and cities powered by the heat thrown off by computer servers These are all stretch goals and that s good said Paul Saffo managing director of foresight at the investment advisory firm Discern in San Francisco In an era when pessimism is the new black a little dose of technological optimism is not a bad thing For IBM it s not just idle speculation The company is one of the few big corporations investing in long range research projects and it counts on innovation to fuel growth Saffo said Not all of its predictions pan out though IBM was overly optimistic about the spread of speech technology for instance When the ideas do lead to products they can have broad implications for society as well as IBM s bottom line he said Research Spending They have continued to do research when all the other grand research organizations are gone said Saffo who is also a consulting associate professor at Stanford University IBM invested 5 8 billion in research and development last year 6 1 percent of revenue While that s down from about 10 percent in the early 1990s the company spends a bigger share on research than its computing rivals Hewlett Packard Co the top maker of personal computers spent 2 4 percent last year At Almaden scientists work on projects that don t always fit in with IBM s computer business The lab s research includes efforts to develop an electric car battery that runs 500 miles on one charge a filtration system for desalination and a program that shows changes in geographic data IBM rose 9 cents to 146 04 at 11 02 a m in New York Stock Exchange composite trading The stock had gained 11 percent this year before today Citizen Science The list is meant to give a window into the company s innovation engine said Josephine Cheng a vice president at IBM s Almaden lab All this demonstrates a real culture of innovation at IBM and willingness to devote itself to solving some of the world s biggest problems she said Many of the predictions are based on projects that IBM has in the works One of this year s ideas that sensors in cars wallets and personal devices will give scientists better data about the environment is an expansion of the company s citizen science initiative Earlier this year IBM teamed up with the California State Water Resources Control Board and the City of San Jose Environmental Services to help gather information about waterways Researchers from Almaden created an application that lets smartphone users snap photos of streams and creeks and report back on conditions The hope is that these casual observations will help local and state officials who don t have the resources to do the work themselves Traffic Predictors IBM also sees data helping shorten commutes in the next five years Computer programs will use algorithms and real time traffic information to predict which roads will have backups and how to avoid getting stuck Batteries may last 10 times longer in 2015 than today IBM says Rather than using the current lithium ion technology new models could rely on energy dense metals that only need to interact with the air to recharge Some electronic devices might ditch batteries altogether and use something similar to kinetic wristwatches which only need to be shaken to generate a charge The final prediction involves recycling the heat generated by computers and data centers Almost half of the power used by data centers is currently spent keeping the computers cool IBM scientists say it would be better to harness that heat to warm houses and offices In IBM s first list of predictions compiled at the end of 2006 researchers said instantaneous speech translation would become the norm That hasn t happened yet While some programs can quickly translate electronic documents and instant messages and other apps can perform limited speech translation there s nothing widely available that acts like the universal translator in Star Trek Second Life The company also predicted that online immersive environments such as Second Life would become more widespread While immersive video games are as popular as ever Second Life s growth has slowed Internet users are flocking instead to the more 2 D environments of Facebook Inc and Twitter Inc Meanwhile a 2007 prediction that mobile phones will act as a wallet ticket broker concierge bank and shopping assistant is coming true thanks to the explosion of smartphone applications Consumers can pay bills through their banking apps buy movie tickets and get instant feedback on potential purchases all with a few taps on their phones The nice thing about the list is that it provokes thought Saffo said If everything came true they wouldn t be doing their job To contact the reporter on this story Ryan Flinn in San Francisco at rflinn bloomberg net To contact the editor responsible for this story Tom Giles at tgiles5 bloomberg net by 2015, your mobile phone will project a 3-d image of anyone who calls and your laptop will be powered by kinetic energy. at least that\\\\u2019s what international business machines corp. sees in its crystal ball.\"\",\"\"url\"\":\"\"bloomberg news 2010 12 23 ibm predicts holographic calls air breathing batteries by 2015 html\"\"}\"',\n",
       "  u'\"business\"',\n",
       "  u'\"0.789131\"',\n",
       "  u'\"2.055555556\"',\n",
       "  u'\"0.676470588\"',\n",
       "  u'\"0.205882353\"',\n",
       "  u'\"0.047058824\"',\n",
       "  u'\"0.023529412\"',\n",
       "  u'\"0.443783175\"',\n",
       "  u'\"0\"',\n",
       "  u'\"0\"',\n",
       "  u'\"0.09077381\"',\n",
       "  u'\"0\"',\n",
       "  u'\"0.245831182\"',\n",
       "  u'\"0.003883495\"',\n",
       "  u'\"1\"',\n",
       "  u'\"1\"',\n",
       "  u'\"24\"',\n",
       "  u'\"0\"',\n",
       "  u'\"5424\"',\n",
       "  u'\"170\"',\n",
       "  u'\"8\"',\n",
       "  u'\"0.152941176\"',\n",
       "  u'\"0.079129575\"',\n",
       "  u'\"0\"'],\n",
       " [u'\"http://www.popsci.com/technology/article/2012-07/electronic-futuristic-starting-gun-eliminates-advantages-races\"',\n",
       "  u'\"8471\"',\n",
       "  u'\"{\"\"title\"\":\"\"The Fully Electronic Futuristic Starting Gun That Eliminates Advantages in Races the fully electronic, futuristic starting gun that eliminates advantages in races the fully electronic, futuristic starting gun that eliminates advantages in races\"\",\"\"body\"\":\"\"And that can be carried on a plane without the hassle too The Omega E Gun Starting Pistol Omega It s easy to take for granted just how insanely close some Olympic races are and how much the minutiae of it all can matter The perfect example is the traditional starting gun Seems easy You pull a trigger and the race starts Boom What people don t consider When a conventional gun goes off the sound travels to the ears of the closest runner a fraction of a second sooner than the others That s just enough to matter and why the latest starting pistol has traded in the mechanical boom for orchestrated electronic noise Omega has been the watch company tasked as the official timekeeper of the Olympic Games since 1932 At the 2010 Vancouver games they debuted their new starting gun which is a far cry from the iconic revolvers associated with early games it s clearly electronic but still more than a button that s pressed to get the show rolling About as far away as you can get probably while still clearly being a starting gun Pull the trigger once and off the Olympians go If it s pressed twice consecutively it signals a false start Working through a speaker system is what eliminates any kind of advantage for athletes It s not a big advantage being close to a gun but the sound of the bullet traveling one meter every three milliseconds could contribute to a win Powder pistols have been connected to a speaker system before but even then runners could react to the sound of the real pistol firing rather than wait for the speaker sounds to reach them This year s setup will have speakers placed equidistant from runners forcing the sound to reach each competitor at exactly the same time It wouldn t be an enormous difference Omega Timing board member Peter H\\\\u00fcrzeler said in an email but when you think about reaction times being measured in tiny fractions of a second placing a speaker behind each lane has eliminated any sort of advantage for any athlete They all hear the start commands and signal at exactly the same moment There s also an ulterior reason for its look In a post September 11th world a gun on its way to a major event is going to raise more than a few TSA eyebrows even if it s a realistic looking fake Rather than deal with that the e gun can be transported while still maintaining the general look of a starting gun But there s still nothing like hearing a starting gun go off at the start of a race more than signaling the runners there s probably some Pavlovian response after more than a century of Olympic games that make people want to hear the real thing not a whiny electronic noise Everyone in the stands at home thankfully will still be getting that The sound is programmable and can be synthesized to sound like almost anything H\\\\u00fcrzeler says but we program it to sound like a pistol it s a way to use the best possible starting technology but to keep a rich tradition alive and that can be carried on a plane without the hassle, too technology,gadgets,london 2012,london olympics,olympics,omega,starting guns,summer olympics,timing,popular science,popsci\"\",\"\"url\"\":\"\"popsci technology article 2012 07 electronic futuristic starting gun eliminates advantages races\"\"}\"',\n",
       "  u'\"recreation\"',\n",
       "  u'\"0.574147\"',\n",
       "  u'\"3.677966102\"',\n",
       "  u'\"0.50802139\"',\n",
       "  u'\"0.288770053\"',\n",
       "  u'\"0.213903743\"',\n",
       "  u'\"0.144385027\"',\n",
       "  u'\"0.468648998\"',\n",
       "  u'\"0\"',\n",
       "  u'\"0\"',\n",
       "  u'\"0.098707403\"',\n",
       "  u'\"0\"',\n",
       "  u'\"0.203489628\"',\n",
       "  u'\"0.088652482\"',\n",
       "  u'\"1\"',\n",
       "  u'\"1\"',\n",
       "  u'\"40\"',\n",
       "  u'\"0\"',\n",
       "  u'\"4973\"',\n",
       "  u'\"187\"',\n",
       "  u'\"9\"',\n",
       "  u'\"0.181818182\"',\n",
       "  u'\"0.125448029\"',\n",
       "  u'\"1\"'],\n",
       " [u'\"http://www.menshealth.com/health/flu-fighting-fruits?cm_mmc=Facebook-_-MensHealth-_-Content-Health-_-FightFluWithFruit\"',\n",
       "  u'\"1164\"',\n",
       "  u'\"{\"\"title\"\":\"\"Fruits that Fight the Flu fruits that fight the flu | cold & flu | men\\'s health\"\",\"\"body\"\":\"\"Apples The most popular source of antioxidants in our diet one apple has an antioxidant effect equivalent to 1 500 mg of vitamin C Apples are loaded with protective flavonoids which may prevent heart disease and cancer Next Papayas With 250 percent of the RDA of vitamin C a papaya can help kick a cold right out of your system The beta carotene and vitamins C and E in papayas reduce inflammation throughout the body lessening the effects of asthma Next Cranberries Cranberries have more antioxidants than other common fruits and veggies One serving has five times the amount in broccoli Cranberries are a natural probiotic enhancing good bacteria levels in the gut and protecting it from foodborne illnesses Next Grapefruit Loaded with vitamin C grapefruit also contains natural compounds called limonoids which can lower cholesterol The red varieties are a potent source of the cancer fighting substance lycopene Next Bananas One of the top food sources of vitamin B6 bananas help reduce fatigue depression stress and insomnia Bananas are high in magnesium which keeps bones strong and potassium which helps prevent heart disease and high blood pressure Next everything you need to know about cold and flu so you don\\\\u2019t get sick this season, at men\\\\u2019s health.com cold, flu, infection, sore throat, sneeze, immunity, germs, allergies, stay healthy, sick, contagious, medicines, cold medicine\"\",\"\"url\"\":\"\"menshealth health flu fighting fruits cm mmc Facebook Mens Health Content Health Fight Flu With Fruit\"\"}\"',\n",
       "  u'\"health\"',\n",
       "  u'\"0.996526\"',\n",
       "  u'\"2.382882883\"',\n",
       "  u'\"0.562015504\"',\n",
       "  u'\"0.321705426\"',\n",
       "  u'\"0.120155039\"',\n",
       "  u'\"0.042635659\"',\n",
       "  u'\"0.525448029\"',\n",
       "  u'\"0\"',\n",
       "  u'\"0\"',\n",
       "  u'\"0.072447859\"',\n",
       "  u'\"0\"',\n",
       "  u'\"0.22640177\"',\n",
       "  u'\"0.120535714\"',\n",
       "  u'\"1\"',\n",
       "  u'\"1\"',\n",
       "  u'\"55\"',\n",
       "  u'\"0\"',\n",
       "  u'\"2240\"',\n",
       "  u'\"258\"',\n",
       "  u'\"11\"',\n",
       "  u'\"0.166666667\"',\n",
       "  u'\"0.057613169\"',\n",
       "  u'\"1\"'],\n",
       " [u'\"http://www.dumblittleman.com/2007/12/10-foolproof-tips-for-better-sleep.html\"',\n",
       "  u'\"6684\"',\n",
       "  u'\"{\"\"title\"\":\"\"10 Foolproof Tips for Better Sleep \"\",\"\"body\"\":\"\"There was a period in my life when I had a lot of problems with sleep It took me very long to fall asleep I was easily awaken and I simply wasn t getting enough of rest at night I didn t want to take medication and this led me to learn several tips and tricks that really helped me to overcome my insomnia Some of these tips I try to follow regularly Don t worry about not getting enough sleep Try not to worry about how much you sleep Such worrying can start a cycle of negative thoughts that contribute to a condition known as learned insomnia Learned insomnia occurs when you worry so much about whether or not you will be able to get adequate sleep that the bedtime rituals and behavior actually trigger insomnia Don t force yourself to sleep The very attempt of trying to do so actually awakes you making it more difficult to sleep Go to bed only when you are feeling really tired and sleepy Don t look at the alarm clock at night Looking at the clock promotes increased anxiety and obsession about time Body heating procedures Some studies suggest that soaking in hot water before going to bed can ease the transition into a deeper sleep Avoid oversleep Don t oversleep to make up for a poor night s sleep Doing so for even a couple of days can reset your body clock and make it harder for you to sleep at night Sex Sex is a well known nighttime stress reliever Healthy sex life enhances your relationship relaxes your body releases happy chemicals and even promotes wellness And it welcomes sleep Avoid alcohol as a sleeping aid Avoid the use of alcohol in the late evening The most common myth found among people is that they believe alcohol helps in the sleep But the fact is alcohol may initially act as sedative but it produces a number of sleep impairing effects in the long run Associate your bed and bedroom with sleep and sex only Don t watch TV eat or read in bed Although these things help some people sleep they can also give your brain the idea that bed isn t just for sleeping and this can keep you awake Naps If you suffer from insomnia try not taking a nap If the goal is to sleep more during the night napping may steal hours desired later on If you re a regular napper and experiencing difficulty falling or staying asleep at night give up the nap and see what happens Written by C Simmons of HealthAssist net dumb little man shares ideas to make the everyday person more productive in life. expect to read tips on finance, saving money, business, and some diy for the house. tips,diy,money,finance,advice,productivity,efficient,technology,saving,software,business,tools\"\",\"\"url\"\":\"\"dumblittleman 2007 12 10 foolproof tips for better sleep html\"\"}\"',\n",
       "  u'\"health\"',\n",
       "  u'\"0.801248\"',\n",
       "  u'\"1.543103448\"',\n",
       "  u'\"0.4\"',\n",
       "  u'\"0.1\"',\n",
       "  u'\"0.016666667\"',\n",
       "  u'\"0\"',\n",
       "  u'\"0.480724749\"',\n",
       "  u'\"0\"',\n",
       "  u'\"0\"',\n",
       "  u'\"0.095860566\"',\n",
       "  u'\"0\"',\n",
       "  u'\"0.265655744\"',\n",
       "  u'\"0.035343035\"',\n",
       "  u'\"1\"',\n",
       "  u'\"0\"',\n",
       "  u'\"24\"',\n",
       "  u'\"0\"',\n",
       "  u'\"2737\"',\n",
       "  u'\"120\"',\n",
       "  u'\"5\"',\n",
       "  u'\"0.041666667\"',\n",
       "  u'\"0.100858369\"',\n",
       "  u'\"1\"']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData = sc.textFile('../data/evergreen_classification/train_noheader.tsv')\n",
    "records = rawData.map(lambda x : x.split('\\t'))\n",
    "records.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]),\n",
       " LabeledPoint(1.0, [0.574147,3.677966102,0.50802139,0.288770053,0.213903743,0.144385027,0.468648998,0.0,0.0,0.098707403,0.0,0.203489628,0.088652482,1.0,1.0,40.0,0.0,4973.0,187.0,9.0,0.181818182,0.125448029]),\n",
       " LabeledPoint(1.0, [0.996526,2.382882883,0.562015504,0.321705426,0.120155039,0.042635659,0.525448029,0.0,0.0,0.072447859,0.0,0.22640177,0.120535714,1.0,1.0,55.0,0.0,2240.0,258.0,11.0,0.166666667,0.057613169]),\n",
       " LabeledPoint(1.0, [0.801248,1.543103448,0.4,0.1,0.016666667,0.0,0.480724749,0.0,0.0,0.095860566,0.0,0.265655744,0.035343035,1.0,0.0,24.0,0.0,2737.0,120.0,5.0,0.041666667,0.100858369]),\n",
       " LabeledPoint(0.0, [0.719157,2.676470588,0.5,0.222222222,0.12345679,0.043209877,0.446143274,0.0,0.0,0.024908425,0.0,0.228887247,0.050473186,1.0,1.0,14.0,0.0,12032.0,162.0,10.0,0.098765432,0.082568807])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "trimmed = records.map(lambda x: [xx.replace('\\\\',' ') for xx in x])\n",
    "# data.first()\n",
    "label = trimmed.map(lambda x : x[-1])\n",
    "# label.take(5)\n",
    "# features =  trimmed.map(lambda x: x[4:-1]).map(lambda x: [ 0.0 if x=='?' else float(xx.replace(\"\\\"\",\"\")) for xx in x])\n",
    "# data = LabeledPoint(label,Vectors.dense(features))\n",
    "# data = trimmed.map(lambda x:(x[-1],x[4:-1])).map(lambda (x,y): (x,[ 0.0 if yy =='?' else float(yy.replace(\"\\\"\",\"\")) for yy in y])).map(LabeledPoint(label,features))\n",
    "# ?号时，文本里面存的是\"?\"\n",
    "data = trimmed.map(lambda x:(x[-1],x[4:-1])).map(lambda (x,y): (x.replace(\"\\\"\",\"\") ,[ 0.0 if yy =='\\\"?\\\"' else yy.replace(\"\\\"\",\"\") for yy in y])).map(lambda (x,y):(int(x),[float(yy) for yy in y])).map(lambda (x,y):LabeledPoint(x,Vectors.dense(y)))\n",
    "# features.take(5)\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]), LabeledPoint(1.0, [0.574147,3.677966102,0.50802139,0.288770053,0.213903743,0.144385027,0.468648998,0.0,0.0,0.098707403,0.0,0.203489628,0.088652482,1.0,1.0,40.0,0.0,4973.0,187.0,9.0,0.181818182,0.125448029]), LabeledPoint(1.0, [0.996526,2.382882883,0.562015504,0.321705426,0.120155039,0.042635659,0.525448029,0.0,0.0,0.072447859,0.0,0.22640177,0.120535714,1.0,1.0,55.0,0.0,2240.0,258.0,11.0,0.166666667,0.057613169]), LabeledPoint(1.0, [0.801248,1.543103448,0.4,0.1,0.016666667,0.0,0.480724749,0.0,0.0,0.095860566,0.0,0.265655744,0.035343035,1.0,0.0,24.0,0.0,2737.0,120.0,5.0,0.041666667,0.100858369]), LabeledPoint(0.0, [0.719157,2.676470588,0.5,0.222222222,0.12345679,0.043209877,0.446143274,0.0,0.0,0.024908425,0.0,0.228887247,0.050473186,1.0,1.0,14.0,0.0,12032.0,162.0,10.0,0.098765432,0.082568807])]\n",
      "7395\n"
     ]
    }
   ],
   "source": [
    "# data.cache()\n",
    "print data.take(5)\n",
    "print data.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]), LabeledPoint(1.0, [0.574147,3.677966102,0.50802139,0.288770053,0.213903743,0.144385027,0.468648998,0.0,0.0,0.098707403,0.0,0.203489628,0.088652482,1.0,1.0,40.0,0.0,4973.0,187.0,9.0,0.181818182,0.125448029]), LabeledPoint(1.0, [0.996526,2.382882883,0.562015504,0.321705426,0.120155039,0.042635659,0.525448029,0.0,0.0,0.072447859,0.0,0.22640177,0.120535714,1.0,1.0,55.0,0.0,2240.0,258.0,11.0,0.166666667,0.057613169]), LabeledPoint(1.0, [0.801248,1.543103448,0.4,0.1,0.016666667,0.0,0.480724749,0.0,0.0,0.095860566,0.0,0.265655744,0.035343035,1.0,0.0,24.0,0.0,2737.0,120.0,5.0,0.041666667,0.100858369]), LabeledPoint(0.0, [0.719157,2.676470588,0.5,0.222222222,0.12345679,0.043209877,0.446143274,0.0,0.0,0.024908425,0.0,0.228887247,0.050473186,1.0,1.0,14.0,0.0,12032.0,162.0,10.0,0.098765432,0.082568807])]\n"
     ]
    }
   ],
   "source": [
    "# naive bayes要求feature为非负features\n",
    "nbdata = trimmed.map(lambda x:(x[-1],x[4:-1])).map(lambda (x,y): (int(x.replace(\"\\\"\",\"\")) ,[ 0.0 if yy =='\\\"?\\\"' else float(yy.replace(\"\\\"\",\"\")) for yy in y])).map(lambda (x,y): (x,[0.0 if yy<0 else yy for yy in y])).map(lambda (x,y):LabeledPoint(x,Vectors.dense(y)))\n",
    "# nbdata = trimmed.map(lambda x:(x[-1],x[4:-1])).map(lambda (x,y): (x.replace(\"\\\"\",\"\") ,[ 0.0 if yy =='\\\"?\\\"' else yy.replace(\"\\\"\",\"\") for yy in y])).map(lambda (x,y):(int(x),[float(yy) for yy in y])).map(lambda (x,y):[0.0 if yy<0  else float(yy) for yy in y]).map(lambda (x,y):LabeledPoint(x,Vectors.dense(y)))\n",
    "print nbdata.take(5)\n",
    "# nbdata.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(weights=[-0.110216274454,-0.493200344739,-0.0712665620384,-0.0214744216778,0.00276706475384,0.00246385887598,-1.33300460292,0.0525232672351,0.0,-0.0320576776,-0.00653638798541,-0.0613702511674,-0.14975863133,-0.13648187383,-0.121161700009,-15.6451616669,-0.0177690355464,745.987958686,-7.73567729685,-1.38587998188,-0.0355600416613,-0.0352085128613], intercept=0.0)\n",
      "DecisionTreeModel classifier of depth 5 with 61 nodes\n"
     ]
    }
   ],
   "source": [
    "# Training a classifier using logistic regression, SVM, naïve Bayes, and a decision tree\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "# import pyspark.mllib.tree.\n",
    "numIteration = 10\n",
    "maxTreeDepth = 5\n",
    "numClass = label.distinct().count()\n",
    "print numClass\n",
    "lrModel = LogisticRegressionWithSGD.train(data, numIteration)\n",
    "svmModel = SVMWithSGD.train(data, numIteration)\n",
    "nbModel = NaiveBayes.train(nbdata)\n",
    "# dtModel = DecisionTree.trainClassifier(data,2,impurity='entropy')\n",
    "dtModel = DecisionTree.trainClassifier(data,numClass,{},impurity='entropy', maxDepth=maxTreeDepth)\n",
    "print lrModel\n",
    "print dtModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The true label is 0.0, and the predict label is 1\n"
     ]
    }
   ],
   "source": [
    "# using these models\n",
    "dataPoint = data.first()\n",
    "prediction = lrModel.predict(dataPoint.features)\n",
    "trueLabel = dataPoint.label\n",
    "print 'The true label is %s, and the predict label is %s'%(trueLabel, prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 1.0, 0.0, 1.0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrPrediction = lrModel.predict(data.map(lambda lp: lp.features))\n",
    "dtPrediction = dtModel.predict(nbdata.map(lambda lp: lp.features))\n",
    "# lrPrediction.take(5)\n",
    "dtPrediction.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------data count: 7395------------\n",
      "------------lr Model Accuracy: 0.514672075727------------\n",
      "------------svm Model Accuracy: 0.514672------------\n",
      "------------nb Model Accuracy: 0.580392------------\n",
      "------------dt Model Accuracy: 0.648276------------\n",
      "-----------------------done-----------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the classifier\n",
    "lrTotalCorrect = data.map(lambda lp : 1 if(lrModel.predict(lp.features)==lp.label) else 0).sum()\n",
    "svmTotalCorrect = data.map(lambda lp : 1 if(svmModel.predict(lp.features)==lp.label) else 0).sum()\n",
    "nbTotalCorrect = nbdata.map(lambda lp: 1 if (nbModel.predict(lp.features) == lp.label) else 0).sum()\n",
    "# dtTotalCorrect = data.map(lambda lp: 1 if (dtModel.predict(lp.features) == lp.label) else 0).sum()\n",
    "# 要查下这里为什么会有问题\n",
    "# dtTotalCorrect = data.map(lambda lp: 1 if (dtModel.predict(lp.features) == lp.label) else 0).sum()\n",
    "# predictionAndLabel = data.map(lambda lp: (dtModel.predict(lp.features),lp.label))\n",
    "# print predictionAndLabel.take(5)\n",
    "# dtTotalCorrect = predictionAndLabel.map(lambda (x,y): 1.0 if x==y else 0.0).sum()\n",
    "# labels = data.map(lambda lp:lp.label).zip(prediction)\n",
    "\n",
    "\n",
    "predictList= dtModel.predict(data.map(lambda lp: lp.features)).collect()\n",
    "trueLabel = data.map(lambda lp: lp.label).collect()\n",
    "# # diff = abs(predictList-trueLabel)\n",
    "dtTotalCorrect = sum([1.0 if predictVal == trueLabel[i] else 0.0 for i, predictVal in enumerate(predictList)])\n",
    "# dtTotalCorrect = sum(diff)\n",
    "# print dtTotalCorrect\n",
    "lrAccuracy = lrTotalCorrect/(data.count()*1.0)\n",
    "svmAccuracy = svmTotalCorrect/(data.count()*1.0)\n",
    "nbAccuracy = nbTotalCorrect/(1.0*nbdata.count())\n",
    "dtAccuracy = dtTotalCorrect/(1.0*data.count())\n",
    "print '------------data count: %s------------'%data.count()\n",
    "print '------------lr Model Accuracy: %s------------'%lrAccuracy\n",
    "print '------------svm Model Accuracy: %f------------'%svmAccuracy\n",
    "print '------------nb Model Accuracy: %f------------'%nbAccuracy\n",
    "print '------------dt Model Accuracy: %f------------'%dtAccuracy\n",
    "print '-----------------------done-----------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LogisticRegressionModel', 0.5014181143280931, 0.7567586293858841), ('SVMModel', 0.5014181143280931, 0.7567586293858841)]\n"
     ]
    }
   ],
   "source": [
    "# 计算AUC分数\n",
    "# import pyspark.mllib.evaluation.BinaryClassificationMetrics\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "# 下面写法不对\n",
    "all_models_metrics = []\n",
    "for model in [lrModel,svmModel]:\n",
    "    scoresAndLabels = data.map(lambda point:(model.predict(point.features),point.label)).collect()\n",
    "    scoresAndLabels = [(float(i),j) for (i,j) in scoresAndLabels]\n",
    "    \n",
    "    scoresAndLabels_sc = sc.parallelize(scoresAndLabels)\n",
    "    metrics = BinaryClassificationMetrics(scoresAndLabels_sc)\n",
    "    all_models_metrics.append((model.__class__.__name__,metrics.areaUnderROC, metrics.areaUnderPR))\n",
    "\n",
    "print all_models_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LogisticRegressionModel', 0.5014181143280931, 0.7567586293858841), ('SVMModel', 0.5014181143280931, 0.7567586293858841), ('NaiveBayesModel', 0.5835585110136261, 0.6808510815151734)]\n"
     ]
    }
   ],
   "source": [
    "for model in [nbModel]:\n",
    "    # float(model.predict(point.features)) is important or get a error \n",
    "    #'DoubleType can not accept object in type <type 'numpy.float64'>'\n",
    "    scoresAndLabels = nbdata.map(lambda point:(float(model.predict(point.features)),point.label)).collect()\n",
    "    #scoresAndLabeles = [(1.0*i,j) for (i,j) in scoresAndLabeles]\n",
    "    #print scoresAndLabeles\n",
    "    scoresAndLabels_sc = sc.parallelize(scoresAndLabels)\n",
    "    #print scoresAndLabeles\n",
    "    scoresAndLabeles_sc = scoresAndLabels_sc\n",
    "    nb_metrics = BinaryClassificationMetrics(scoresAndLabels_sc)\n",
    "    all_models_metrics.append((model.__class__.__name__, nb_metrics.areaUnderROC, nb_metrics.areaUnderPR))\n",
    "print all_models_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LogisticRegressionModel', 0.5014181143280931, 0.7567586293858841), ('SVMModel', 0.5014181143280931, 0.7567586293858841), ('NaiveBayesModel', 0.5835585110136261, 0.6808510815151734), ('DecisionTreeModel', 0.6488371887050935, 0.7430805993331199)]\n"
     ]
    }
   ],
   "source": [
    "for model in [dtModel]:\n",
    "#     scoresAndLabeles = data.map(lambda point:(model.predict(point.features),point.label)).collect()\n",
    "    predictList= dtModel.predict(data.map(lambda lp: lp.features)).collect()\n",
    "    trueLabel = data.map(lambda lp: lp.label).collect()\n",
    "#     scoresAndLabeles = [(1.0*i,j) for (i,j) in scoresAndLabeles]\n",
    "#     print scoresAndLabeles\n",
    "    scoresAndLabels = [(predictList[i],true_val) for i, true_val in enumerate(trueLabel)]\n",
    "    scoresAndLabels_sc = sc.parallelize(scoresAndLabels)\n",
    "#     print scoresAndLabeles\n",
    "    scoresAndLabels_sc = scoresAndLabels_sc.map(lambda (x,y): (float(x),float(y)))\n",
    "    dt_metrics = BinaryClassificationMetrics(scoresAndLabels_sc)\n",
    "    all_models_metrics.append((model.__class__.__name__, dt_metrics.areaUnderROC, dt_metrics.areaUnderPR))\n",
    "    \n",
    "print all_models_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Improving model performance and tuning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - didn't throw all our data at the model\n",
    " - need do a lot of analysis on these numeric features\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Feature standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4.12258053e-01   2.76182319e+00   4.68230473e-01   2.14079926e-01\n",
      "   9.20623607e-02   4.92621604e-02   2.25510345e+00  -1.03750428e-01\n",
      "   0.00000000e+00   5.64227450e-02   2.12305612e-02   2.33778177e-01\n",
      "   2.75709037e-01   6.15551048e-01   6.60311021e-01   3.00770791e+01\n",
      "   3.97565923e-02   5.71659824e+03   1.78754564e+02   4.96064909e+00\n",
      "   1.72864050e-01   1.01220792e-01]\n",
      "[  8.01248000e-01   1.54310345e+00   4.00000000e-01   1.00000000e-01\n",
      "   1.66666670e-02   0.00000000e+00   4.80724749e-01   0.00000000e+00\n",
      "   0.00000000e+00   9.58605660e-02   0.00000000e+00   2.65655744e-01\n",
      "   3.53430350e-02   1.00000000e+00   0.00000000e+00   2.40000000e+01\n",
      "   0.00000000e+00   2.73700000e+03   1.20000000e+02   5.00000000e+00\n",
      "   4.16666670e-02   1.00858369e-01]\n",
      "[  4.46119000e-01   2.39743590e+00   6.50000000e-01   3.62500000e-01\n",
      "   3.75000000e-02   2.50000000e-02   2.30207065e-01   0.00000000e+00\n",
      "   0.00000000e+00   8.79478830e-02   0.00000000e+00   2.27856821e-01\n",
      "   1.38888890e-02   1.00000000e+00   1.00000000e+00   4.50000000e+01\n",
      "   0.00000000e+00   1.01300000e+03   8.00000000e+01   7.00000000e+00\n",
      "   7.50000000e-02   6.17283950e-02]\n",
      "7395\n",
      "[  1.09727602e-01   7.42907773e+01   4.12575900e-02   2.15305244e-02\n",
      "   9.21057177e-03   5.27422016e-03   3.25347870e+01   9.39571798e-02\n",
      "   0.00000000e+00   1.71750875e-03   2.07798245e-02   2.75446690e-03\n",
      "   3.68329077e+00   2.36647955e-01   2.24300377e-01   4.15822321e+02\n",
      "   3.81760057e-02   7.87626486e+07   3.22037609e+04   1.04515955e+01\n",
      "   3.35890913e-02   6.27668400e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AttributeError: \"'PipelinedRDD' object has no attribute '_detach'\" in <bound method MultivariateStatisticalSummary.__del__ of <pyspark.mllib.stat._statistics.MultivariateStatisticalSummary object at 0xb08b688c>> ignored\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import MultivariateStatisticalSummary\n",
    "vectors = data.map(lambda lp : lp.features)\n",
    "matrix = MultivariateStatisticalSummary(vectors)\n",
    "matrix_mean = matrix.mean()\n",
    "matrix_min = matrix.min()\n",
    "print matrix_mean\n",
    "print matrix_min\n",
    "print matrix.max()\n",
    "print matrix.count()\n",
    "print matrix.variance()\n",
    "# print matrix.numNonzeros()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import StandardScalerModel,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(withMean=True, withStd=True).fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DenseVector([0.7891, 2.0556, 0.6765, 0.2059, 0.0471, 0.0235, 0.4438, 0.0, 0.0, 0.0908, 0.0, 0.2458, 0.0039, 1.0, 1.0, 24.0, 0.0, 5424.0, 170.0, 8.0, 0.1529, 0.0791]), DenseVector([0.5741, 3.678, 0.508, 0.2888, 0.2139, 0.1444, 0.4686, 0.0, 0.0, 0.0987, 0.0, 0.2035, 0.0887, 1.0, 1.0, 40.0, 0.0, 4973.0, 187.0, 9.0, 0.1818, 0.1254]), DenseVector([0.9965, 2.3829, 0.562, 0.3217, 0.1202, 0.0426, 0.5254, 0.0, 0.0, 0.0724, 0.0, 0.2264, 0.1205, 1.0, 1.0, 55.0, 0.0, 2240.0, 258.0, 11.0, 0.1667, 0.0576]), DenseVector([0.8012, 1.5431, 0.4, 0.1, 0.0167, 0.0, 0.4807, 0.0, 0.0, 0.0959, 0.0, 0.2657, 0.0353, 1.0, 0.0, 24.0, 0.0, 2737.0, 120.0, 5.0, 0.0417, 0.1009]), DenseVector([0.7192, 2.6765, 0.5, 0.2222, 0.1235, 0.0432, 0.4461, 0.0, 0.0, 0.0249, 0.0, 0.2289, 0.0505, 1.0, 1.0, 14.0, 0.0, 12032.0, 162.0, 10.0, 0.0988, 0.0826])]\n",
      "[1.1376473365,-0.0819355716929,1.02513981289,-0.0558635644254,-0.468893253129,-0.354305326308,-0.317535217236,0.33845079824,0.0,0.828822173315,-0.147268943346,0.229639823578,-0.141625969099,0.790238049918,0.717194729453,-0.297996816496,-0.20346257793,-0.0329672096969,-0.0487811297558,0.940069975117,-0.108698488525,-0.278820782314]\n"
     ]
    }
   ],
   "source": [
    "labels = data.map(lambda lp: lp.label)\n",
    "features = data.map(lambda lp: lp.features)\n",
    "print features.take(5)\n",
    "scaled_data = labels.zip(scaler.transform(features))\n",
    "scaled_data = scaled_data.map(lambda (x,y): LabeledPoint(x,y))\n",
    "print scaled_data.first().features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]\n"
     ]
    }
   ],
   "source": [
    "print data.first().features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用归一化的数据来重新使用lr模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrAccuracyscaled : 0.620960\n"
     ]
    }
   ],
   "source": [
    "lrModelScaled = LogisticRegressionWithSGD.train(scaled_data, numIteration)\n",
    "lrTotalCorrectScaled = scaled_data.map(lambda lp : 1 if(lrModelScaled.predict(lp.features)==lp.label) else 0).sum()\n",
    "lrAccuracyScaled = lrTotalCorrectScaled/(1.0*data.count())\n",
    "print 'lrAccuracyscaled : %f'%lrAccuracyScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LogisticRegressionModel', 0.6201898373011353, 0.7277006868096805)]\n"
     ]
    }
   ],
   "source": [
    "all_models_metrics =[]\n",
    "for model in [lrModelScaled]:\n",
    "    scoresAndLabels = scaled_data.map(lambda point:(model.predict(point.features),point.label)).collect()\n",
    "    scoresAndLabels = [(float(i),j) for (i,j) in scoresAndLabels]\n",
    "    \n",
    "    scoresAndLabels_sc = sc.parallelize(scoresAndLabels)\n",
    "    metrics = BinaryClassificationMetrics(scoresAndLabels_sc)\n",
    "    all_models_metrics.append((model.__class__.__name__,metrics.areaUnderROC, metrics.areaUnderPR))\n",
    "\n",
    "print all_models_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Adding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = records.map(lambda x: x[3]).distinct().zipWithIndex().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'\"recreation\"', 0),\n",
       " (u'\"gaming\"', 1),\n",
       " (u'\"arts_entertainment\"', 2),\n",
       " (u'\"computer_internet\"', 3),\n",
       " (u'\"?\"', 4),\n",
       " (u'\"sports\"', 5),\n",
       " (u'\"business\"', 6),\n",
       " (u'\"health\"', 7),\n",
       " (u'\"science_technology\"', 8),\n",
       " (u'\"religion\"', 9),\n",
       " (u'\"law_crime\"', 10),\n",
       " (u'\"unknown\"', 11),\n",
       " (u'\"culture_politics\"', 12),\n",
       " (u'\"weather\"', 13)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_dict = {}\n",
    "categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  (x,y) in [(key.replace('\\\"','') ,val) for (key, val) in categories]:\n",
    "    category_dict[x] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'?': 4,\n",
       " u'arts_entertainment': 2,\n",
       " u'business': 6,\n",
       " u'computer_internet': 3,\n",
       " u'culture_politics': 12,\n",
       " u'gaming': 1,\n",
       " u'health': 7,\n",
       " u'law_crime': 10,\n",
       " u'recreation': 0,\n",
       " u'religion': 9,\n",
       " u'science_technology': 8,\n",
       " u'sports': 5,\n",
       " u'unknown': 11,\n",
       " u'weather': 13}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(category_dict)\n",
    "num_categories = len(category_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]),\n",
       " LabeledPoint(1.0, [0.574147,3.677966102,0.50802139,0.288770053,0.213903743,0.144385027,0.468648998,0.0,0.0,0.098707403,0.0,0.203489628,0.088652482,1.0,1.0,40.0,0.0,4973.0,187.0,9.0,0.181818182,0.125448029]),\n",
       " LabeledPoint(1.0, [0.996526,2.382882883,0.562015504,0.321705426,0.120155039,0.042635659,0.525448029,0.0,0.0,0.072447859,0.0,0.22640177,0.120535714,1.0,1.0,55.0,0.0,2240.0,258.0,11.0,0.166666667,0.057613169]),\n",
       " LabeledPoint(1.0, [0.801248,1.543103448,0.4,0.1,0.016666667,0.0,0.480724749,0.0,0.0,0.095860566,0.0,0.265655744,0.035343035,1.0,0.0,24.0,0.0,2737.0,120.0,5.0,0.041666667,0.100858369]),\n",
       " LabeledPoint(0.0, [0.719157,2.676470588,0.5,0.222222222,0.12345679,0.043209877,0.446143274,0.0,0.0,0.024908425,0.0,0.228887247,0.050473186,1.0,1.0,14.0,0.0,12032.0,162.0,10.0,0.098765432,0.082568807])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otherdata = trimmed.map(lambda x:(x[-1],x[4:-1])).map(lambda (x,y): (x.replace(\"\\\"\",\"\") ,[ 0.0 if yy =='\\\"?\\\"' else yy.replace(\"\\\"\",\"\") for yy in y])).map(lambda (x,y):(int(x),[float(yy) for yy in y])).map(lambda (x,y):LabeledPoint(x,Vectors.dense(y)))\n",
    "otherdata.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]),\n",
       " LabeledPoint(1.0, [1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.574147,3.677966102,0.50802139,0.288770053,0.213903743,0.144385027,0.468648998,0.0,0.0,0.098707403,0.0,0.203489628,0.088652482,1.0,1.0,40.0,0.0,4973.0,187.0,9.0,0.181818182,0.125448029]),\n",
       " LabeledPoint(1.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.996526,2.382882883,0.562015504,0.321705426,0.120155039,0.042635659,0.525448029,0.0,0.0,0.072447859,0.0,0.22640177,0.120535714,1.0,1.0,55.0,0.0,2240.0,258.0,11.0,0.166666667,0.057613169]),\n",
       " LabeledPoint(1.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.801248,1.543103448,0.4,0.1,0.016666667,0.0,0.480724749,0.0,0.0,0.095860566,0.0,0.265655744,0.035343035,1.0,0.0,24.0,0.0,2737.0,120.0,5.0,0.041666667,0.100858369]),\n",
       " LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.719157,2.676470588,0.5,0.222222222,0.12345679,0.043209877,0.446143274,0.0,0.0,0.024908425,0.0,0.228887247,0.050473186,1.0,1.0,14.0,0.0,12032.0,162.0,10.0,0.098765432,0.082568807])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def func1(x):\n",
    "    import numpy as np\n",
    "    label = x[-1].replace('\\\"','')\n",
    "    other_feature = [0.0 if yy == '?' else yy for yy in [ y.replace('\\\"','') for y in x[4:-1]]]\n",
    "    category_Idx = category_dict[x[3].replace('\\\"','')]\n",
    "    category_feature = np.zeros(num_categories)\n",
    "    category_feature[category_Idx] = 1\n",
    "    return LabeledPoint(label, Vectors.dense(list(category_feature)+other_feature))\n",
    "category_data = trimmed.map(lambda x:func1(x))\n",
    "category_data.take(5)\n",
    "# category_data.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7891, 2.0556, 0.6765, 0.2059, 0.0471, 0.0235, 0.4438, 0.0, 0.0, 0.0908, 0.0, 0.2458, 0.0039, 1.0, 1.0, 24.0, 0.0, 5424.0, 170.0, 8.0, 0.1529, 0.0791]), DenseVector([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5741, 3.678, 0.508, 0.2888, 0.2139, 0.1444, 0.4686, 0.0, 0.0, 0.0987, 0.0, 0.2035, 0.0887, 1.0, 1.0, 40.0, 0.0, 4973.0, 187.0, 9.0, 0.1818, 0.1254]), DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965, 2.3829, 0.562, 0.3217, 0.1202, 0.0426, 0.5254, 0.0, 0.0, 0.0724, 0.0, 0.2264, 0.1205, 1.0, 1.0, 55.0, 0.0, 2240.0, 258.0, 11.0, 0.1667, 0.0576]), DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8012, 1.5431, 0.4, 0.1, 0.0167, 0.0, 0.4807, 0.0, 0.0, 0.0959, 0.0, 0.2657, 0.0353, 1.0, 0.0, 24.0, 0.0, 2737.0, 120.0, 5.0, 0.0417, 0.1009]), DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7192, 2.6765, 0.5, 0.2222, 0.1235, 0.0432, 0.4461, 0.0, 0.0, 0.0249, 0.0, 0.2289, 0.0505, 1.0, 1.0, 14.0, 0.0, 12032.0, 162.0, 10.0, 0.0988, 0.0826])]\n",
      "[LabeledPoint(0.0, [-0.446421204794,-0.101894690972,-0.381813223243,-0.204182210579,-0.680752790425,-0.232727977095,2.72073665645,-0.270999069693,-0.201654052319,-0.0991499193088,-0.0648775723926,-0.028494000387,-0.220526884579,-0.0232621058984,1.1376473365,-0.0819355716929,1.02513981289,-0.0558635644254,-0.468893253129,-0.354305326308,-0.317535217236,0.33845079824,0.0,0.828822173315,-0.147268943346,0.229639823578,-0.141625969099,0.790238049918,0.717194729453,-0.297996816496,-0.20346257793,-0.0329672096969,-0.0487811297558,0.940069975117,-0.108698488525,-0.278820782314]), LabeledPoint(1.0, [2.23973405107,-0.101894690972,-0.381813223243,-0.204182210579,-0.680752790425,-0.232727977095,-0.367497813919,-0.270999069693,-0.201654052319,-0.0991499193088,-0.0648775723926,-0.028494000387,-0.220526884579,-0.0232621058984,0.488685990417,0.106283637051,0.195885662909,0.508986806825,1.26946916328,1.30971389846,-0.313176090577,0.33845079824,0.0,1.02024383053,-0.147268943346,-0.577072420563,-0.0974598108014,0.790238049918,0.717194729453,0.486582251769,-0.20346257793,-0.0837816352001,0.0459442290216,1.24936955983,0.0488534204631,0.305780221901]), LabeledPoint(1.0, [-0.446421204794,-0.101894690972,-0.381813223243,-0.204182210579,-0.680752790425,-0.232727977095,-0.367497813919,3.68955057532,-0.201654052319,-0.0991499193088,-0.0648775723926,-0.028494000387,-0.220526884579,-0.0232621058984,1.76370015145,-0.0439616503324,0.461691874163,0.733429729796,0.292698491466,-0.0912380098151,-0.303218882637,0.33845079824,0.0,0.386653800111,-0.147268943346,-0.140538950608,-0.0808480767065,0.790238049918,0.717194729453,1.22212512827,-0.20346257793,-0.391710293516,0.441561903916,1.86796872925,-0.0338127019291,-0.550386680318]), LabeledPoint(1.0, [-0.446421204794,-0.101894690972,-0.381813223243,-0.204182210579,-0.680752790425,-0.232727977095,-0.367497813919,3.68955057532,-0.201654052319,-0.0991499193088,-0.0648775723926,-0.028494000387,-0.220526884579,-0.0232621058984,1.1742243126,-0.141386202438,-0.335890011943,-0.777414366305,-0.785550085109,-0.678273674121,-0.311059139608,0.33845079824,0.0,0.951555374196,-0.147268943346,0.607347191147,-0.125234986113,0.790238049918,-1.39413290761,-0.297996816496,-0.20346257793,-0.335713021952,-0.32738512616,0.0121712209805,-0.715808218485,-0.00457426292901]), LabeledPoint(0.0, [-0.446421204794,-0.101894690972,-0.381813223243,-0.204182210579,-0.680752790425,4.29628094558,-0.367497813919,-0.270999069693,-0.201654052319,-0.0991499193088,-0.0648775723926,-0.028494000387,-0.220526884579,-0.0232621058984,0.926420355743,-0.00990193242163,0.156397371928,0.0554868659863,0.327099538145,-0.0833318004825,-0.317121477845,0.33845079824,0.0,-0.760377215897,-0.147268943346,-0.0931844125948,-0.117351906844,0.790238049918,0.717194729453,-0.788358734162,-0.20346257793,0.711559893907,-0.0933577691805,1.55866914454,-0.404279404638,-0.235413062338])]\n"
     ]
    }
   ],
   "source": [
    "category_labels = category_data.map(lambda lp: lp.label)\n",
    "category_features = category_data.map(lambda lp: lp.features)\n",
    "scaler2 = StandardScaler(withMean=True, withStd=True).fit(category_features)\n",
    "print category_features.take(5)\n",
    "scaled_category_data = category_labels.zip(scaler2.transform(category_features))\n",
    "scaled_category_data = scaled_category_data.map(lambda (x,y): LabeledPoint(x,y))\n",
    "print scaled_category_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrModel_category_scaled : 0.665720\n"
     ]
    }
   ],
   "source": [
    "lrModel_category_scaled = LogisticRegressionWithSGD.train(scaled_category_data, numIteration)\n",
    "lr_totalCorrect_category_scaled = scaled_category_data.map(lambda lp : 1 if(lrModel_category_scaled.predict(lp.features)==lp.label) else 0).sum()\n",
    "lr_accuracy_category_scaled = lr_totalCorrect_category_scaled/(1.0*data.count())\n",
    "print 'lrModel_category_scaled : %f'%lr_accuracy_category_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LogisticRegressionModel', 0.665475474542015, 0.7579822657619434)]\n"
     ]
    }
   ],
   "source": [
    "all_models_metrics =[]\n",
    "for model in [lrModel_category_scaled]:\n",
    "    scoresAndLabels = scaled_category_data.map(lambda point:(model.predict(point.features),point.label)).collect()\n",
    "    scoresAndLabels = [(float(i),j) for (i,j) in scoresAndLabels]\n",
    "    \n",
    "    scoresAndLabels_sc = sc.parallelize(scoresAndLabels)\n",
    "    metrics = BinaryClassificationMetrics(scoresAndLabels_sc)\n",
    "    all_models_metrics.append((model.__class__.__name__,metrics.areaUnderROC, metrics.areaUnderPR))\n",
    "\n",
    "print all_models_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the correct form of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_category_nb = scaled_category_data\n",
    "data_category_nb_nonegative = data_category_nb.map(lambda x: (x.label,x.features)).map(lambda (x,y): (x,[0.0 if yy<0.0 else yy for yy in y])).map(lambda (x,y):LabeledPoint(x,Vectors.dense(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------nb Model Accuracy: 0.652738------------\n"
     ]
    }
   ],
   "source": [
    "nb_category_model = NaiveBayes.train(data_category_nb_nonegative)\n",
    "nb_category_total_correct = data_category_nb_nonegative.map(lambda lp: 1 if (nb_category_model.predict(lp.features) == lp.label) else 0).sum()\n",
    "\n",
    "nb_category_accuracy = nb_category_total_correct/(1.0*data_category_nb_nonegative.count())\n",
    "print '------------nb Model Accuracy: %f------------'%nb_category_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NaiveBayesModel', 0.6514679174141278, 0.7520379357367744)]\n"
     ]
    }
   ],
   "source": [
    "all_models_metrics =[]\n",
    "for model in [nb_category_model]:\n",
    "    scoresAndLabels = data_category_nb_nonegative.map(lambda point:(model.predict(point.features),point.label)).collect()\n",
    "    scoresAndLabels = [(float(i),j) for (i,j) in scoresAndLabels]\n",
    "    \n",
    "    scoresAndLabels_sc = sc.parallelize(scoresAndLabels)\n",
    "    metrics = BinaryClassificationMetrics(scoresAndLabels_sc)\n",
    "    all_models_metrics.append((model.__class__.__name__,metrics.areaUnderROC, metrics.areaUnderPR))\n",
    "\n",
    "print all_models_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###调参\n",
    "大家都知道，其实跑一个模型不是难点，在机器学习中，最难的是如何为模型选择最优的参数\n",
    "下面就为大家演示在spark中我们如何做这部分的工作\n",
    "pyspark甚至比scala在这部分表现更优秀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_params(input, reg_param, num_iter, step_size):\n",
    "    lr_model = LogisticRegressionWithSGD.train(input,iterations=num_iter, regParam=reg_param, step=step_size)\n",
    "    return lr_model\n",
    "def create_metrics(tag, data, model):\n",
    "    score_labels = data.map(lambda x: (model.predict(x.features)*1.0,x.label*1.0))\n",
    "#     score_labels_sc = sc.parallelize(score_labels)\n",
    "    metrics = BinaryClassificationMetrics(score_labels)\n",
    "    return tag,metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iterations,AUC = 64.95%\n",
      "5 iterations,AUC = 66.62%\n",
      "10 iterations,AUC = 66.55%\n",
      "50 iterations,AUC = 66.81%\n"
     ]
    }
   ],
   "source": [
    "for i in [1,5,10,50]:\n",
    "    model = train_with_params(scaled_category_data, 0.0, i, 1.0)\n",
    "    label, roc = create_metrics('%d iterations'%i,scaled_category_data,model)\n",
    "    print '%s,AUC = %2.2f%%'%(label,roc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001000 step size,AUC = 64.97%\n",
      "0.010000 step size,AUC = 64.96%\n",
      "0.100000 step size,AUC = 65.52%\n",
      "1.000000 step size,AUC = 66.55%\n",
      "10.000000 step size,AUC = 61.92%\n"
     ]
    }
   ],
   "source": [
    "for s in [0.001, 0.01, 0.1, 1.0, 10.0]:\n",
    "    model = train_with_params(scaled_category_data, 0.0, 10, s)\n",
    "    label, roc = create_metrics('%f step size'%s,scaled_category_data,model)\n",
    "    print '%s,AUC = %2.2f%%'%(label,roc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001000 regularization parameter,AUC = 64.95%\n",
      "0.010000 regularization parameter,AUC = 64.95%\n",
      "0.100000 regularization parameter,AUC = 64.95%\n",
      "1.000000 regularization parameter,AUC = 64.95%\n",
      "10.000000 regularization parameter,AUC = 64.95%\n"
     ]
    }
   ],
   "source": [
    "for r in [0.001, 0.01, 0.1, 1.0, 10.0]:\n",
    "    model = train_with_params(scaled_category_data, 0.0, 1.0, r)\n",
    "    label, roc = create_metrics('%f regularization parameter'%r,scaled_category_data,model)\n",
    "    print '%s,AUC = %2.2f%%'%(label,roc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_params_dt(input, impurity, maxTreeDepth):\n",
    "    dt_model = DecisionTree.trainClassifier(input,numClass,{},impurity, maxDepth=maxTreeDepth)\n",
    "    return dt_model\n",
    "def create_metrics_dt(tag, data, model):\n",
    "    predictList= model.predict(data.map(lambda lp: lp.features)).collect()\n",
    "    trueLabel = data.map(lambda lp: lp.label).collect()\n",
    "    scoresAndLabels = [(predictList[i],true_val) for i, true_val in enumerate(trueLabel)]\n",
    "    scoresAndLabels_sc = sc.parallelize(scoresAndLabels)\n",
    "    scoresAndLabels_sc = scoresAndLabels_sc.map(lambda (x,y): (float(x),float(y)))\n",
    "    dt_metrics = BinaryClassificationMetrics(scoresAndLabels_sc)\n",
    "    return tag,dt_metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impurity: entropy, 1 maxTreeDepth:, AUC = 59.33\n",
      "impurity: gini, 1 maxTreeDepth:, AUC = 59.33\n",
      "impurity: entropy, 2 maxTreeDepth:, AUC = 61.68\n",
      "impurity: gini, 2 maxTreeDepth:, AUC = 61.68\n",
      "impurity: entropy, 3 maxTreeDepth:, AUC = 62.61\n",
      "impurity: gini, 3 maxTreeDepth:, AUC = 62.61\n",
      "impurity: entropy, 4 maxTreeDepth:, AUC = 63.63\n",
      "impurity: gini, 4 maxTreeDepth:, AUC = 63.63\n",
      "impurity: entropy, 5 maxTreeDepth:, AUC = 64.88\n",
      "impurity: gini, 5 maxTreeDepth:, AUC = 64.89\n",
      "impurity: entropy, 10 maxTreeDepth:, AUC = 76.26\n",
      "impurity: gini, 10 maxTreeDepth:, AUC = 78.37\n",
      "impurity: entropy, 20 maxTreeDepth:, AUC = 98.45\n",
      "impurity: gini, 20 maxTreeDepth:, AUC = 98.87\n"
     ]
    }
   ],
   "source": [
    "for dep in [1,2,3,4,5,10,20]:\n",
    "    for im in ['entropy','gini']:\n",
    "        model=train_with_params_dt(data,im,dep)\n",
    "        tag, roc = create_metrics_dt('impurity: %s, %d maxTreeDepth:'%(im,dep),data,model)\n",
    "        print '%s, AUC = %2.2f'%(tag,roc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_params_nb(input, lambda1):\n",
    "    nb_model = NaiveBayes.train(input,lambda1)\n",
    "    return nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_metrics_nb(tag, nbbata, model):\n",
    "    scoresAndLabels = nbdata.map(lambda point:(float(model.predict(point.features)),point.label))\n",
    "    nb_metrics = BinaryClassificationMetrics(scoresAndLabels)\n",
    "    return tag,nb_metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001000 lambda, AUC = 58.37\n",
      "0.010000 lambda, AUC = 58.37\n",
      "0.100000 lambda, AUC = 58.37\n",
      "1.000000 lambda, AUC = 58.36\n",
      "10.000000 lambda, AUC = 58.34\n"
     ]
    }
   ],
   "source": [
    "for la in [0.001, 0.01, 0.1, 1.0, 10.0]:\n",
    "    model=train_with_params_nb(nbdata,la)\n",
    "    tag, roc = create_metrics_dt('%f lambda' %la,data,model)\n",
    "    print '%s, AUC = %2.2f'%(tag,roc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000 regularization parameter,AUC = 50.00%\n",
      "0.001000 regularization parameter,AUC = 64.44%\n",
      "0.002500 regularization parameter,AUC = 64.44%\n",
      "0.005000 regularization parameter,AUC = 64.44%\n",
      "0.010000 regularization parameter,AUC = 64.44%\n"
     ]
    }
   ],
   "source": [
    "train_test_split = scaled_category_data.randomSplit([0.6,0.4],123)\n",
    "train = train_test_split[0]\n",
    "test = train_test_split[1]\n",
    "for r in [0.0, 0.001, 0.0025, 0.005, 0.01]:\n",
    "    model = train_with_params(train, 0.0, 1.0, r)\n",
    "    label, roc = create_metrics('%f regularization parameter'%r,test,model)\n",
    "    print '%s,AUC = %2.2f%%'%(label,roc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
